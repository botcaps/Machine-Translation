{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/anaconda3/envs/machine_translation/lib/python3.10/site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#  %pip install torchinfo\n",
    "\n",
    "#comment it along with the code for building the intuition and articulative manners that's mean whenever I read code I am able to understand the structure and code \n",
    "# write a markdown for intuition and articulation what I did in this cell and how it is going to work and why it is important and when we have to use this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy==2.0\n",
    "# %pip install \"numpy<2.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                                # PyTorch main package for tensor operations and deep learning\n",
    "import torch.nn as nn                                       # PyTorch module for building neural network layers\n",
    "import torch.optim as optim                                 # PyTorch module for optimization algorithms (e.g., Adam, SGD)\n",
    "import sacrebleu                                            # Library for calculating BLEU score (translation quality metric)\n",
    "from torchtext.data.utils import get_tokenizer              # Utility to get tokenizers for text preprocessing\n",
    "from torchtext.vocab import build_vocab_from_iterator       # Function to build vocabulary from tokenized data\n",
    "from torchtext.datasets import Multi30k                     # Multi30k dataset for English-German/French translation tasks\n",
    "from typing import Tuple                                    # Type hinting for functions that return tuples\n",
    "import spacy                                                # NLP library for tokenization and linguistic features\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)                #check the version of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the Multi-Head Self-Attention mechanism.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): Total dimension of the model (e.g., 512). This is the size of input embeddings.\n",
    "        num_heads (int): Number of attention heads to split the model into.\n",
    "\n",
    "        Concept:\n",
    "        Instead of performing one large attention, we divide it into multiple 'heads'.\n",
    "        Each head learns to focus on different parts of the input using smaller dimensional spaces (head_dim = d_model / num_heads).\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Ensure d_model is divisible evenly among all heads\n",
    "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Linear layers to project input into queries (Q), keys (K), and values (V)\n",
    "        # These transform the input into three separate learned representations\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final linear layer to project concatenated heads' output back to d_model\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Head Self-Attention.\n",
    "\n",
    "        Args:\n",
    "        q (Tensor): Query tensor (batch_size, seq_len, d_model)\n",
    "        k (Tensor): Key tensor (batch_size, seq_len, d_model)\n",
    "        v (Tensor): Value tensor (batch_size, seq_len, d_model)\n",
    "        mask (Tensor, optional): Attention mask to block certain positions (like padding or future tokens in decoding)\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output after applying multi-head attention (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Step 1: Project input into query, key, and value vectors\n",
    "        # Then reshape each into multiple heads for parallel attention\n",
    "        # Output shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Step 2: Compute scaled dot-product attention\n",
    "        # Attention score = Q . K^T / sqrt(d_k)\n",
    "        # This measures how much each word should attend to others\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 3 (Optional): Apply mask if provided\n",
    "        # Useful to prevent attention to certain tokens (like padding or future tokens)\n",
    "        if mask is not None:\n",
    "            # Mask out unwanted positions with very negative value (-inf) so they become 0 after softmax\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Step 4: Normalize scores to probabilities using softmax\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 5: Weighted sum of value vectors using attention weights\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Step 6: Concatenate all heads' output together\n",
    "        # Transpose and reshape back to (batch_size, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Step 7: Final linear layer to mix the heads and produce the output\n",
    "        out = self.wo(attn_output)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Multi-Head Self-Attention: Intuition, Role, and Importance\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented the **Multi-Head Self-Attention (MHSA)** module from scratch using PyTorch. This block is one of the most critical components of the Transformer architecture. The goal of this module is to allow the model to attend to different parts of the sequence **in parallel**, capturing richer relationships between words or tokens.\n",
    "\n",
    "Key steps involved:\n",
    "- Projected the input embeddings into **Query (Q)**, **Key (K)**, and **Value (V)** vectors.\n",
    "- Split the embedding into multiple \"attention heads\" to allow the model to learn different attention patterns.\n",
    "- Performed **scaled dot-product attention** across each head.\n",
    "- Applied optional **masking** for decoder/causal attention or padding.\n",
    "- Concatenated the results from all heads and passed them through a final linear layer.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind Multi-Head Attention\n",
    "Imagine reading a sentence like:\n",
    "> \"The animal didn't cross the road because it was too tired.\"\n",
    "\n",
    "To understand what \"it\" refers to, your brain simultaneously considers:\n",
    "- Recent nouns (\"animal\", \"road\")\n",
    "- Verb context (\"cross\", \"was\")\n",
    "- Grammatical cues\n",
    "\n",
    "**Multi-head attention mimics this by allowing the model to look at the input from multiple perspectives at once.** Each head can focus on a different type of dependency ‚Äî e.g., subject-verb, object-adjective, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Internally\n",
    "1. The input tensor is projected into Q, K, and V vectors.\n",
    "2. Each head computes attention using `Q √ó K^T / sqrt(d_k)` to get how much focus should be given to each token.\n",
    "3. These scores are passed through a softmax to get attention weights.\n",
    "4. We then take a weighted sum of the value vectors using those attention weights.\n",
    "5. Finally, we concatenate all head outputs and project back to the model's original dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "- Enables the model to capture **contextual relationships** across all words in the input sequence, regardless of their position.\n",
    "- Improves expressiveness over single-head attention by capturing **multiple types of interactions** in parallel.\n",
    "- Crucial for **language understanding, translation, summarization**, and almost all modern NLP tasks.\n",
    "\n",
    "Without multi-head attention, the model would be limited to a single way of attending, making it less flexible and less powerful.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When To Use This\n",
    "Use Multi-Head Self-Attention when:\n",
    "- You're building a **Transformer-based model** (e.g., for NLP or Vision Transformers).\n",
    "- You need to capture **complex, long-range dependencies** between sequence elements.\n",
    "- You're implementing **machine translation, text classification, summarization, question answering**, etc.\n",
    "- You want to build a model that treats every position in a sequence with **equal access** to other positions, rather than relying on sequential recurrence (like RNNs).\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Key Parameters\n",
    "- `d_model`: Dimension of the model (e.g., 512 or 768)\n",
    "- `num_heads`: Number of parallel attention heads (e.g., 8 or 12)\n",
    "- `mask`: Optional masking to restrict attention to certain positions (used heavily in the decoder)\n",
    "\n",
    "---\n",
    "\n",
    "In summary, this cell is not just a building block‚Äîit's the **core engine** behind the Transformer‚Äôs ability to ‚Äúpay attention‚Äù to relevant context in a smart and parallelized way. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position- wise Feed Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Position-wise Feed-Forward module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): The input/output dimensionality of the model (e.g., 512).\n",
    "        - d_ff (int): The dimensionality of the intermediate hidden layer (e.g., 2048).\n",
    "        - dropout (float): Probability of dropout for regularization.\n",
    "\n",
    "        üìå Intuition:\n",
    "        This module is applied **independently to each position** (token) in the sequence.\n",
    "        It acts like a mini fully-connected neural network that gives the model **non-linearity** and **depth**.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "\n",
    "        # First linear layer expands the dimension from d_model to d_ff\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "\n",
    "        # Dropout helps prevent overfitting by randomly zeroing some of the activations\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Second linear layer projects back from d_ff to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Position-wise Feed-Forward layer.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Input tensor with shape (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output tensor with the same shape (batch_size, seq_length, d_model)\n",
    "\n",
    "        ‚öôÔ∏è Step-by-step:\n",
    "        1. Expand the dimensionality of each token's embedding (like a hidden layer).\n",
    "        2. Apply ReLU non-linearity to introduce non-linear transformations.\n",
    "        3. Apply dropout to add regularization.\n",
    "        4. Project the representation back to the original d_model dimension.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Apply first linear layer\n",
    "        out = self.linear1(x)\n",
    "\n",
    "        # Step 2: Apply ReLU non-linearity element-wise\n",
    "        out = F.relu(out)\n",
    "\n",
    "        # Step 3: Apply dropout for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Step 4: Project back to d_model dimensions\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented the **Position-wise Feed-Forward Network** ‚Äî a critical component used inside every Transformer block. Unlike attention layers that operate across different positions in the sequence, this module operates **independently on each position** (i.e., token) in the input sequence.\n",
    "\n",
    "What happens in this cell:\n",
    "- I created a two-layer feed-forward neural network.\n",
    "- The first layer expands the dimensionality from `d_model` to a larger `d_ff` (usually 2048).\n",
    "- Applied ReLU activation to introduce non-linearity.\n",
    "- Used dropout for regularization.\n",
    "- Projected it back to the original `d_model` dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind Position-wise FFN\n",
    "In the Transformer architecture, self-attention allows tokens to **interact with each other** and exchange context. However, we still need a mechanism to allow each token to **process its own representation** and extract features **non-linearly**.\n",
    "\n",
    "This is where the Position-wise FFN comes in:\n",
    "- It acts like a **miniature MLP (Multi-Layer Perceptron)** applied to each token's embedding.\n",
    "- This helps the model **refine and transform the information** that each token receives after attention.\n",
    "\n",
    "The idea is: **\"Let each token process its contextualized meaning individually.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "For each token embedding (i.e., a vector of size `d_model`), the FFN performs:\n",
    "1. A linear transformation to a higher-dimensional space (`d_ff`).\n",
    "2. A ReLU activation to add non-linearity.\n",
    "3. Dropout to reduce overfitting.\n",
    "4. Another linear transformation back to `d_model`.\n",
    "\n",
    "This process happens **independently for every token** in the batch and sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "- **Adds Depth & Non-Linearity**: The Transformer would be too shallow and linear without this module.\n",
    "- **Token-wise Learning**: While attention focuses on relationships between tokens, the FFN improves the **representation of each token individually**.\n",
    "- **Flexible Transformation**: Gives the model more expressive power to capture complex patterns in token embeddings.\n",
    "- **Weight Sharing**: The same FFN is applied at each position ‚Äî saving memory while keeping consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "Use this module:\n",
    "- As a **standard component inside each Transformer block**, right after the self-attention and residual connection.\n",
    "- When building models like **BERT, GPT, T5, etc.**\n",
    "- Any time you need to **refine token-level representations after attention**, especially in tasks like:\n",
    "  - Machine translation\n",
    "  - Text classification\n",
    "  - Summarization\n",
    "  - Question answering\n",
    "\n",
    "---\n",
    "\n",
    "In summary, this cell builds a simple yet powerful transformation layer that ensures each token is not just \"context-aware\" (via attention) but also **individually smart** and **non-linearly enhanced**. üß©üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the AddNorm module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): The dimensionality of the input tensor (e.g., 512 or 768).\n",
    "        - eps (float): A small constant to ensure numerical stability in LayerNorm.\n",
    "\n",
    "        üìå Intuition:\n",
    "        This module implements the \"Add & Norm\" step used in Transformer blocks.\n",
    "        It combines a **residual connection** with **layer normalization** to help the model train efficiently and stay stable.\n",
    "        \"\"\"\n",
    "        super(AddNorm, self).__init__()\n",
    "\n",
    "        # Layer normalization normalizes each feature vector (along last dim) to have mean 0 and variance 1\n",
    "        # It helps with training deep models and accelerates convergence\n",
    "        self.norm = nn.LayerNorm(d_model, eps=eps)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        \"\"\"\n",
    "        Forward pass through Add & Norm.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Output tensor from the sublayer (e.g., self-attention or feed-forward).\n",
    "                     Shape: (batch_size, seq_len, d_model)\n",
    "        - residual (Tensor): The original input to the sublayer, which is added back (residual connection).\n",
    "                             Same shape as x.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Normalized output after adding the residual.\n",
    "                  Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        ‚öôÔ∏è Step-by-step:\n",
    "        1. Add the residual connection: This helps prevent vanishing gradients and retains original information.\n",
    "        2. Apply LayerNorm to stabilize the training and keep the scale of activations controlled.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Add input (residual connection)\n",
    "        out = x + residual  # Preserve original signal + apply new transformation\n",
    "\n",
    "        # Step 2: Normalize the result\n",
    "        out = self.norm(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Add & Norm: Residual Connections with Layer Normalization\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented the **AddNorm** module, which performs two critical operations found in every block of a Transformer:\n",
    "1. **Residual (Skip) Connection**: Adds the original input (`residual`) back to the sublayer output (`x`).\n",
    "2. **Layer Normalization**: Normalizes the combined output to stabilize the learning process.\n",
    "\n",
    "This is known as the **\"Add & Norm\"** step in the original Transformer architecture and is used both after the self-attention block and after the feed-forward block.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind Add & Norm\n",
    "Deep neural networks can suffer from vanishing gradients and unstable training, especially when stacked in many layers.\n",
    "\n",
    "The solution:\n",
    "- **Residual connections** help preserve the original input signal and make gradient flow easier, reducing the risk of degradation in deep networks.\n",
    "- **Layer normalization** ensures that the outputs of each layer maintain a stable distribution (zero mean, unit variance), which helps the optimizer converge faster and makes training more stable.\n",
    "\n",
    "Together, they allow each layer to:\n",
    "- Learn a residual mapping instead of a full transformation.\n",
    "- Avoid exploding or vanishing activations during training.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Step-by-Step\n",
    "1. **Residual Addition**:\n",
    "   - `x + residual`: Combines the transformed representation (`x`) with the original input.\n",
    "   - This helps the model learn the *change* needed instead of the full transformation.\n",
    "2. **Layer Normalization**:\n",
    "   - Normalizes each embedding (feature vector per token) across the last dimension.\n",
    "   - Keeps values on a similar scale, improves convergence, and prevents instability.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "- Prevents degradation in very deep models by simplifying learning.\n",
    "- Speeds up convergence by keeping distributions stable.\n",
    "- Maintains contextual signals from earlier layers, which is important in long sequences.\n",
    "- Essential in virtually **all Transformer-based models**, including BERT, GPT, T5, and more.\n",
    "\n",
    "Without \"Add & Norm\", the deep stacking of layers in the Transformer would result in poor gradient flow and unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "Use AddNorm:\n",
    "- After **multi-head self-attention** layers.\n",
    "- After **position-wise feed-forward** layers.\n",
    "- Whenever you want to **retain original features** while still benefiting from transformations.\n",
    "- In any model that relies on **deep layer stacking** and **sequential context**, especially:\n",
    "  - Transformers (NLP & Vision)\n",
    "  - Encoders / Decoders\n",
    "  - BERT, GPT, T5, ViT, etc.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, this cell implements a powerful design pattern that supports both **stability** and **depth** in the Transformer architecture ‚Äî helping the model learn efficiently without losing valuable contextual information. üß©üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimensionality of the embeddings (e.g., 512).\n",
    "        - max_seq_len (int): Maximum length of the input sequence.\n",
    "        - dropout (float): Dropout probability to prevent overfitting.\n",
    "\n",
    "        üìå Intuition:\n",
    "        Since Transformers don‚Äôt have any built-in notion of word order (unlike RNNs or CNNs),\n",
    "        we inject information about token positions using **positional encodings**.\n",
    "        These are fixed, learnable patterns added to the input embeddings to help the model \n",
    "        understand **\"who came before/after whom\"** in a sequence.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Dropout for regularization (optional but common in Transformer models)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Step 1: Initialize positional encoding matrix (shape: [max_seq_len, d_model])\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # Step 2: Generate a list of positions [0, 1, 2, ..., max_seq_len-1] as a column vector\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Step 3: Compute the denominator term for sinusoidal frequency scaling\n",
    "        # This ensures different frequencies for each dimension\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Step 4: Apply sine to even indices (2i) and cosine to odd indices (2i+1)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # shape: [max_seq_len, d_model/2]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # shape: [max_seq_len, d_model/2]\n",
    "\n",
    "        # Step 5: Add a batch dimension to match input shape (1, max_seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_seq_len, d_model)\n",
    "\n",
    "        # Step 6: Register this as a buffer so it's saved with the model but not updated by gradients\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for PositionalEncoding.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Input embeddings of shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output embeddings with positional information added.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 7: Add positional encodings to input embeddings (broadcast across batch dimension)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "        # Step 8: Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Positional Encoding: Giving Transformers a Sense of Order\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented the **PositionalEncoding** module, which adds **position information** to input token embeddings in a Transformer model.\n",
    "\n",
    "Unlike RNNs or CNNs, Transformers process all tokens **in parallel** and have **no built-in understanding of order**. To solve this, we inject position information using deterministic **sinusoidal positional encodings**, which are then **added** to the input embeddings.\n",
    "\n",
    "This module:\n",
    "- Precomputes a fixed positional encoding matrix using sine and cosine functions.\n",
    "- Adds it to the token embeddings before feeding them into the attention mechanism.\n",
    "- Applies dropout to prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind Positional Encoding\n",
    "Transformers treat all tokens equally ‚Äî the word \"dog\" at position 1 is indistinguishable from \"dog\" at position 8.\n",
    "\n",
    "But in language, **position matters**:\n",
    "- ‚ÄúHe ate before sleeping‚Äù ‚â† ‚ÄúHe slept before eating.‚Äù\n",
    "\n",
    "So, we inject position information by **encoding each position as a unique vector** using sine and cosine functions. These vectors:\n",
    "- Vary smoothly across positions.\n",
    "- Allow the model to infer relative and absolute positions using mathematical properties.\n",
    "- Are **not learned**, making them lightweight and generalizable.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "1. Create a positional encoding matrix of shape `(max_seq_len, d_model)`.\n",
    "2. For each position:\n",
    "   - Use `sin(pos / 10000^(2i/d_model))` for even indices.\n",
    "   - Use `cos(pos / 10000^(2i/d_model))` for odd indices.\n",
    "3. Add this positional encoding to the input embeddings (element-wise addition).\n",
    "4. Apply dropout for regularization.\n",
    "\n",
    "This results in **embeddings that contain both the content (word meaning)** and **the position (order)** of each token in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "- **Injects positional information** into a model that otherwise has none.\n",
    "- Maintains **parallelism** (unlike RNNs) by not using recurrence.\n",
    "- Helps the model learn **sequence-aware attention** ‚Äî critical for language tasks.\n",
    "- The use of **sinusoids allows the model to generalize to longer sequences** than it was trained on.\n",
    "\n",
    "Without positional encodings, a Transformer would treat a shuffled sentence the same as the original ‚Äî which is undesirable.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "Use PositionalEncoding:\n",
    "- At the very beginning of a Transformer model, **right after word embeddings**.\n",
    "- In both **encoder** and **decoder** parts of the architecture.\n",
    "- In any model that uses **self-attention and parallel token processing** without recurrence.\n",
    "- Common in:\n",
    "  - Machine Translation\n",
    "  - Summarization\n",
    "  - Text Classification\n",
    "  - Vision Transformers (ViT uses learnable positional encodings)\n",
    "\n",
    "---\n",
    "\n",
    "In summary, this cell equips the Transformer with a **sense of token order** ‚Äî an essential component for making meaning from sequences. Without this, the model would be **powerful, but blind to structure**. üß†üìè‚ö°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderBlock module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimensionality of input embeddings (e.g., 512).\n",
    "        - num_heads (int): Number of attention heads in multi-head self-attention.\n",
    "        - d_ff (int): Dimensionality of hidden layer in the feed-forward network.\n",
    "        - dropout (float): Dropout rate to avoid overfitting.\n",
    "\n",
    "        üìå Intuition:\n",
    "        This block forms one layer of the Transformer encoder. Each block contains:\n",
    "        1. Multi-head self-attention to capture contextual relationships.\n",
    "        2. Add & Norm to stabilize and preserve gradients.\n",
    "        3. Position-wise feed-forward network for token-wise transformation.\n",
    "        4. Another Add & Norm to wrap the FFN.\n",
    "\n",
    "        All of this is applied **in parallel** to every token in the input sequence.\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Multi-head self-attention sublayer\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "        # LayerNorm + residual connection after attention\n",
    "        self.norm1 = AddNorm(d_model)\n",
    "\n",
    "        # Position-wise feed-forward sublayer\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # LayerNorm + residual connection after feed-forward\n",
    "        self.norm2 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        - mask (Tensor, optional): Attention mask (e.g., for padding)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output tensor of same shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        ‚öôÔ∏è Step-by-step:\n",
    "        1. Apply self-attention across the sequence (tokens attend to each other).\n",
    "        2. Add the result to the input (residual) and normalize.\n",
    "        3. Feed through position-wise FFN for non-linearity and transformation.\n",
    "        4. Add and normalize again.\n",
    "\n",
    "        Each encoder block refines the representation by deepening and enriching token-level information.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Self-attention (each token attends to all others, including itself)\n",
    "        x1 = self.self_attn(x, x, x, mask)\n",
    "\n",
    "        # Step 2: Add & Norm (residual connection + layer normalization)\n",
    "        x = self.norm1(x, x1)\n",
    "\n",
    "        # Step 3: Position-wise feed-forward network\n",
    "        x1 = self.ffn(x)\n",
    "\n",
    "        # Step 4: Add & Norm again\n",
    "        x = self.norm2(x, x1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Transformer Encoder Block: Self-Attention + FFN + Add & Norm\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented the core **EncoderBlock** used in the Transformer architecture.\n",
    "\n",
    "This block is a composition of the following submodules:\n",
    "1. **Multi-Head Self-Attention** ‚Äì enables each token to attend to every other token in the sequence.\n",
    "2. **Add & Norm** ‚Äì adds a residual connection followed by layer normalization to stabilize training.\n",
    "3. **Position-wise Feed-Forward Network** ‚Äì applies a small neural network independently to each token.\n",
    "4. **Another Add & Norm** ‚Äì ensures stability and smooth gradient flow after the FFN.\n",
    "\n",
    "This design is modular and stackable ‚Äî multiple such blocks can be layered to form a deep encoder.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind EncoderBlock\n",
    "Each token in a sentence should not be understood in isolation. It needs to gather context from other words.\n",
    "\n",
    "The encoder block helps with this by:\n",
    "- **First** letting each token \"attend\" to others via self-attention.\n",
    "- **Then** using a feed-forward network to transform each token individually.\n",
    "- **Both steps** are wrapped in residual connections and layer normalization to make deep learning feasible and efficient.\n",
    "\n",
    "It's like:\n",
    "> \"Listen to everyone else, update your understanding, then process your own meaning ‚Äî repeat.\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Step-by-Step\n",
    "1. **Multi-Head Self-Attention**:\n",
    "   - Computes attention across the sequence using multiple parallel attention heads.\n",
    "   - Helps capture diverse contextual relationships (e.g., syntactic, semantic).\n",
    "\n",
    "2. **Add & Norm 1**:\n",
    "   - Adds the original input back to the attention output (residual connection).\n",
    "   - Applies layer normalization to stabilize training and maintain scale.\n",
    "\n",
    "3. **Feed-Forward Network**:\n",
    "   - Applies a two-layer neural network to each token independently.\n",
    "   - Introduces non-linearity and deeper transformations for each token.\n",
    "\n",
    "4. **Add & Norm 2**:\n",
    "   - Again uses a residual connection and normalization to retain information and improve gradient flow.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "This block is the **building unit of the Transformer encoder**.\n",
    "- Enables **contextual understanding** of each token with respect to others.\n",
    "- Balances **global interaction (via attention)** and **local transformation (via FFN)**.\n",
    "- The use of **residual connections + layer norm** helps train very deep models efficiently.\n",
    "\n",
    "Without this block, the model would struggle to understand word relationships, learn deep patterns, or generalize across long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "Use this EncoderBlock:\n",
    "- As part of a **Transformer Encoder** (e.g., in BERT, T5, ViT).\n",
    "- When you want to **extract rich representations** from an input sequence.\n",
    "- In tasks like:\n",
    "  - **Machine Translation** (encoder side)\n",
    "  - **Text Classification**\n",
    "  - **Summarization**\n",
    "  - **Speech and Vision tasks** using Transformer-based architectures\n",
    "\n",
    "---\n",
    "\n",
    "In summary, this block captures the **essence of what makes Transformers powerful**: deep context-aware reasoning combined with stable and efficient training. üß†‚ö°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DecoderBlock module.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): Dimensionality of input embeddings (e.g., 512).\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        - d_ff (int): Hidden layer size for feed-forward network.\n",
    "        - dropout (float): Dropout rate to avoid overfitting.\n",
    "\n",
    "        üìå Intuition:\n",
    "        The Decoder Block is designed to generate tokens one at a time during inference.\n",
    "        It processes the already-generated output tokens (target sequence) by:\n",
    "        1. Attending to previously generated tokens (via **masked self-attention**),\n",
    "        2. Attending to the encoder‚Äôs outputs (via **encoder-decoder attention**),\n",
    "        3. Transforming the token representations using a **position-wise feed-forward network**.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # 1Ô∏è‚É£ Masked Multi-Head Self-Attention: attends only to earlier positions in the target sequence\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = AddNorm(d_model)\n",
    "\n",
    "        # 2Ô∏è‚É£ Encoder-Decoder Attention: lets decoder attend to encoder outputs\n",
    "        self.enc_dec_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm2 = AddNorm(d_model)\n",
    "\n",
    "        # 3Ô∏è‚É£ Feed-Forward Network: non-linear transformation per token\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Target sequence input (batch_size, tgt_seq_len, d_model)\n",
    "        - enc_output (Tensor): Encoder output (batch_size, src_seq_len, d_model)\n",
    "        - src_mask (Tensor, optional): Encoder mask (e.g., padding)\n",
    "        - tgt_mask (Tensor, optional): Decoder mask (e.g., causal mask)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output tensor (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        ‚öôÔ∏è Step-by-step:\n",
    "        1. Apply masked self-attention so decoder can only look at past tokens.\n",
    "        2. Apply encoder-decoder attention so decoder can focus on relevant input tokens.\n",
    "        3. Transform each token using feed-forward network.\n",
    "        4. Apply residual connections + normalization after each sub-layer.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1Ô∏è‚É£ Masked Self-Attention: prevents attending to future tokens during training\n",
    "        attn1 = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, attn1)\n",
    "\n",
    "        # 2Ô∏è‚É£ Encoder-Decoder Attention: decoder attends to encoder‚Äôs output to guide generation\n",
    "        attn2 = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x, attn2)\n",
    "\n",
    "        # 3Ô∏è‚É£ Feed-Forward Network: enrich each token‚Äôs features\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x, ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Transformer Decoder Block: Intuition and Architecture\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "In this cell, I implemented a **Decoder Block**, which forms one layer of the Transformer decoder. Each decoder block is designed to **generate sequences step by step**, attending to both:\n",
    "- The **already generated tokens** (via masked self-attention), and\n",
    "- The **encoded input sequence** (via encoder-decoder attention).\n",
    "\n",
    "The block consists of three main sublayers, each followed by an Add & Norm operation:\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Encoder-Decoder Multi-Head Attention**\n",
    "3. **Feed-Forward Network**\n",
    "\n",
    "Each sublayer is wrapped in a residual connection and normalized to ensure stability during deep training.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind the Decoder Block\n",
    "The goal of a decoder block is to **predict the next token** in a sequence during training and inference. To do this effectively:\n",
    "- It needs to **look at the previously generated tokens** ‚Äî but **not future tokens** ‚Äî hence the use of **masked self-attention**.\n",
    "- It must also **refer back to the encoder output**, i.e., what it has \"understood\" from the input sentence.\n",
    "- Finally, it transforms each token's features using a small neural network (FFN) to enhance the learned representations.\n",
    "\n",
    "This design allows the model to generate coherent, contextual, and input-aware outputs in tasks like translation, summarization, and dialogue generation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works (Step-by-Step)\n",
    "1. **Masked Multi-Head Self-Attention**:\n",
    "   - The decoder attends to previously generated tokens (causal mask prevents peeking ahead).\n",
    "   - Ensures that predictions are made **autoregressively**.\n",
    "\n",
    "2. **Add & Norm**:\n",
    "   - Applies a residual connection (original input + attention output).\n",
    "   - Followed by layer normalization to stabilize training.\n",
    "\n",
    "3. **Encoder-Decoder Attention**:\n",
    "   - Allows the decoder to attend to the **relevant parts of the input sequence**.\n",
    "   - This cross-attention helps the decoder align input and output sequences.\n",
    "\n",
    "4. **Second Add & Norm**:\n",
    "   - Again, we apply a residual connection and normalization for stability.\n",
    "\n",
    "5. **Feed-Forward Network (FFN)**:\n",
    "   - Applies a two-layer MLP to each token individually.\n",
    "   - Adds depth and non-linearity to the model.\n",
    "\n",
    "6. **Final Add & Norm**:\n",
    "   - Final stabilization before passing output to the next decoder block or output head.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This is Important\n",
    "- **Masked attention enforces autoregressive behavior** during training and inference.\n",
    "- **Cross-attention creates alignment** between input and output ‚Äî essential for translation and sequence mapping tasks.\n",
    "- **Residuals + normalization make training deep networks feasible and efficient**.\n",
    "- **Layer stacking improves depth and expressiveness**, allowing the model to learn complex linguistic patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "Use a Decoder Block:\n",
    "- In any **Transformer-based decoder model**, such as:\n",
    "  - Machine translation (e.g., English to German)\n",
    "  - Summarization\n",
    "  - Dialogue generation\n",
    "  - Image captioning (paired with vision encoders)\n",
    "- In models like **GPT**, **T5**, **Transformer-Decoder-only** architectures, or **seq2seq** systems.\n",
    "\n",
    "It‚Äôs a core module in **auto-regressive sequence generation**.\n",
    "\n",
    "---\n",
    "\n",
    "In short, this Decoder Block enables the Transformer to **generate tokens step-by-step**, learn from both **context and input**, and maintain **stability** across many layers ‚Äî making it a cornerstone of modern NLP and sequence modeling. üß±‚ö°üß†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "Now that we have implemented all the building blocks, let's assemble the complete Transformer architecture.\n",
    "\n",
    "We initialize the following components:\n",
    "\n",
    "- Source and target embedding layers\n",
    "- Positional encoding module\n",
    "- Encoder and decoder layer stacks\n",
    "- Final linear layer to produce the probability distribution over the target vocabulary\n",
    "\n",
    "In the forward method, we first pass the source and target input tensors through their respective embedding layers and add the positional encoding. Then, we pass the source input through each encoder layer sequentially, followed by passing the target input and encoder output through each decoder layer sequentially. Finally, we apply the linear layer to produce the output tensor with shape (batch_size, tgt_seq_length, tgt_vocab_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, D_MODEL, num_heads, d_ff, max_seq_len, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer module.\n",
    "\n",
    "        Args:\n",
    "        src_vocab_size (int): The size of the source vocabulary.\n",
    "        tgt_vocab_size (int): The size of the target vocabulary.\n",
    "        d_model (int): The dimensionality of the embedding\n",
    "        num_heads (int): The number of attention heads.\n",
    "        d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
    "        max_seq_len (int): The maximum length of the input sequence.\n",
    "        num_layers (int): The number of layers in the encoder and decoder.\n",
    "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, D_MODEL)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, D_MODEL)\n",
    "        self.pos_encoding = PositionalEncoding(D_MODEL, max_seq_len, dropout)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(D_MODEL, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer.\n",
    "\n",
    "        Args:\n",
    "        src (Tensor): The source input tensor of shape (batch_size, src_seq_length).\n",
    "        tgt (Tensor): The target input tensor of shape (batch_size, tgt_seq_length).\n",
    "        src_mask (Tensor, optional): The source mask tensor for ignoring certain elements. Defaults to None.\n",
    "        tgt_mask (Tensor, optional): The target mask tensor for ignoring certain elements. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output tensor of shape (batch_size, tgt_seq_length, tgt_vocab_size).\n",
    "        \"\"\"\n",
    "        src = self.src_embedding(src)\n",
    "        src = self.pos_encoding(src)\n",
    "\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
    "\n",
    "        out = self.fc(tgt)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Complete Transformer Architecture: Encoder-Decoder Design\n",
    "\n",
    "### üîß What I Did in This Cell\n",
    "In this cell, I built the **complete Transformer model** by integrating:\n",
    "- **Embedding layers** for both source and target vocabularies.\n",
    "- **Positional encoding** to inject order information.\n",
    "- A stack of `EncoderBlock`s to process the input sequence.\n",
    "- A stack of `DecoderBlock`s to generate the output sequence autoregressively.\n",
    "- A final **linear layer** to project the decoder outputs into vocabulary logits for prediction.\n",
    "\n",
    "This forms a **sequence-to-sequence (seq2seq)** model that can translate, summarize, or generate text.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind the Transformer Model\n",
    "The Transformer model uses a pure **attention-based architecture** (no recurrence or convolution) to model sequences. The core idea is:\n",
    "- The **encoder** reads the input (e.g., an English sentence) and generates a deep contextual representation.\n",
    "- The **decoder** reads this representation while attending to its own generated tokens (e.g., in German) to produce the final output one token at a time.\n",
    "\n",
    "Thanks to **self-attention**, **residual connections**, and **parallelism**, this architecture is extremely effective for language understanding and generation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How the Transformer Works\n",
    "\n",
    "1. **Embeddings + Positional Encoding**:\n",
    "   - Input and output tokens are first converted into dense vectors via embedding layers.\n",
    "   - Positional encoding is added to help the model understand token order.\n",
    "\n",
    "2. **Encoder Stack**:\n",
    "   - A series of `EncoderBlock`s process the source sequence.\n",
    "   - Each block refines the token representation by capturing global dependencies via self-attention.\n",
    "\n",
    "3. **Decoder Stack**:\n",
    "   - A series of `DecoderBlock`s generate the output sequence.\n",
    "   - Each block uses:\n",
    "     - **Masked self-attention** to prevent looking at future tokens.\n",
    "     - **Encoder-decoder attention** to align with the input.\n",
    "     - **Feed-forward layers** for nonlinear transformation.\n",
    "\n",
    "4. **Final Linear Layer**:\n",
    "   - The decoder output (of shape `(batch_size, tgt_seq_len, d_model)`) is passed through a linear layer to convert it into **logits over the target vocabulary**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This is Important\n",
    "\n",
    "- The Transformer is the **foundation of modern NLP and generative AI models**.\n",
    "- Enables **parallel computation** during training (unlike RNNs).\n",
    "- Can model **long-range dependencies** more effectively than traditional sequence models.\n",
    "- Scales well to very large datasets and model sizes (e.g., GPT, BERT, T5).\n",
    "- General-purpose: works for text translation, summarization, question answering, and even vision and protein modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When to Use a Transformer\n",
    "\n",
    "Use this architecture when you need:\n",
    "- **Sequence-to-sequence modeling** (e.g., machine translation, text generation, dialogue systems).\n",
    "- **Parallelism and speed** during training (compared to RNNs or LSTMs).\n",
    "- **Flexibility** in modeling long-range context and complex relationships in data.\n",
    "- A base for **pretraining** large language models (e.g., GPT, BERT, T5).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "This `Transformer` class is a modular, flexible, and powerful implementation of the original encoder-decoder Transformer from the \"Attention is All You Need\" paper. It serves as a foundation for building and customizing many state-of-the-art deep learning models for language, vision, and multimodal tasks. ‚öôÔ∏èüöÄüìö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/v9/8nb03hqj04gds418ch6xw74r0000gn/T/ipykernel_47227/4261485096.py\", line 1, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py\", line 161, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/ticker.py\", line 143, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_transformer\u001b[39m(num_encoder_layers, num_decoder_layers, num_heads, d_model):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py:161\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/rcsetup.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/colors.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/scale.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/ticker.py:143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    145\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    147\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/transforms.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_transformer(num_encoder_layers, num_decoder_layers, num_heads, d_model):\n",
    "    \"\"\"\n",
    "    Visualizes a high-level architecture of the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        num_encoder_layers (int): Number of encoder layers.\n",
    "        num_decoder_layers (int): Number of decoder layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_model (int): Embedding dimension.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Encoder blocks\n",
    "    encoder_x = 1\n",
    "    encoder_y_start = 1\n",
    "    encoder_height = 0.6\n",
    "    encoder_gap = 0.15\n",
    "    for i in range(num_encoder_layers):\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (encoder_x, encoder_y_start + i * (encoder_height + encoder_gap)),\n",
    "            1.2, encoder_height,\n",
    "            boxstyle=\"round,pad=0.02\",\n",
    "            edgecolor='navy', facecolor='#cce5ff', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(encoder_x + 0.6, encoder_y_start + i * (encoder_height + encoder_gap) + encoder_height/2,\n",
    "                f'Encoder Layer {i+1}\\n(Multi-Head x{num_heads})', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Decoder blocks\n",
    "    decoder_x = 5\n",
    "    decoder_y_start = 1\n",
    "    decoder_height = 0.6\n",
    "    decoder_gap = 0.15\n",
    "    for i in range(num_decoder_layers):\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (decoder_x, decoder_y_start + i * (decoder_height + decoder_gap)),\n",
    "            1.2, decoder_height,\n",
    "            boxstyle=\"round,pad=0.02\",\n",
    "            edgecolor='darkgreen', facecolor='#d4edda', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(decoder_x + 0.6, decoder_y_start + i * (decoder_height + decoder_gap) + decoder_height/2,\n",
    "                f'Decoder Layer {i+1}\\n(Multi-Head x{num_heads})', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Embedding and Positional Encoding\n",
    "    ax.text(encoder_x - 0.7, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2,\n",
    "            'Input\\nEmbedding\\n+\\nPositional\\nEncoding', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#f8d7da\", ec=\"crimson\"))\n",
    "    ax.text(decoder_x - 0.7, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2,\n",
    "            'Output\\nEmbedding\\n+\\nPositional\\nEncoding', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#fff3cd\", ec=\"#856404\"))\n",
    "\n",
    "    # Output Linear Layer\n",
    "    ax.text(decoder_x + 2.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2,\n",
    "            'Linear\\n& Softmax', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#d1ecf1\", ec=\"#0c5460\"))\n",
    "\n",
    "    # Arrows: Input -> Encoder\n",
    "    ax.annotate('', xy=(encoder_x, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                xytext=(encoder_x - 0.2, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Arrows: Encoder -> Decoder (cross attention)\n",
    "    ax.annotate('', xy=(encoder_x + 1.2, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                xytext=(decoder_x, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='gray', arrowstyle='->', lw=2, linestyle='dashed'))\n",
    "\n",
    "    # Arrows: Output Embedding -> Decoder\n",
    "    ax.annotate('', xy=(decoder_x, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                xytext=(decoder_x - 0.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Arrows: Decoder -> Linear\n",
    "    ax.annotate('', xy=(decoder_x + 1.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                xytext=(decoder_x + 2.0, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Title and legend\n",
    "    ax.set_title(f\"Transformer Architecture\\n(Encoder Layers: {num_encoder_layers}, Decoder Layers: {num_decoder_layers}, Heads: {num_heads}, d_model: {d_model})\", fontsize=14, pad=20)\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 3 + max(num_encoder_layers, num_decoder_layers) * (encoder_height + encoder_gap) / 2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_transformer(num_encoder_layers=6, num_decoder_layers=6, num_heads=8, d_model=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the **Transformer Architecture**, a foundational model used in many modern NLP systems such as BERT, GPT, and T5. It visualizes how information flows through the **encoder-decoder structure** of a transformer.\n",
    "\n",
    "Let‚Äôs break it down **intuitively and step by step**:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† High-Level Summary\n",
    "\n",
    "* The **left side** is the **Encoder**, which reads and understands the input.\n",
    "* The **right side** is the **Decoder**, which generates the output (like a translated sentence).\n",
    "* In between, the encoder and decoder **communicate** using attention mechanisms.\n",
    "* The **transformer uses 8 attention heads** and **6 stacked layers** in both encoder and decoder, with a model dimension (`d_model`) of 512.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ Step 1: Input Embedding + Positional Encoding\n",
    "\n",
    "* Each word/token in the input sequence is **converted to a dense vector (embedding)**.\n",
    "* Since transformers don‚Äôt understand order, **positional encoding** is added to inject the notion of word order (e.g., who comes first in the sentence).\n",
    "\n",
    "üì¶ Example:\n",
    "\n",
    "> `\"I love transformers\"` ‚Üí becomes a matrix of shape `(seq_len, 512)`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Encoder Stack (Left Side)\n",
    "\n",
    "There are **6 identical encoder layers**, each with:\n",
    "\n",
    "1. **Multi-head self-attention**\n",
    "   ‚Üí Every word looks at every other word (including itself) to understand context.\n",
    "\n",
    "   > e.g., ‚Äúbank‚Äù can mean money or river ‚Äî attention helps disambiguate it by context.\n",
    "\n",
    "2. **Feedforward network**\n",
    "   ‚Üí A small neural network to refine each word‚Äôs representation.\n",
    "\n",
    "3. **Residual connections + LayerNorm**\n",
    "   ‚Üí Helps in stabilizing training and preserving input signals.\n",
    "\n",
    "üìå Output: A context-enriched representation for each word.\n",
    "\n",
    "---\n",
    "\n",
    "## üü° Output Embedding + Positional Encoding\n",
    "\n",
    "This is for the **decoder input** (often previous tokens during training or inference).\n",
    "\n",
    "* The decoder also needs **positional info**.\n",
    "* It uses **shifted right** sequences during training (i.e., we don't feed the full output at once, only up to the current word).\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Decoder Stack (Right Side)\n",
    "\n",
    "Also has **6 layers**, and each contains:\n",
    "\n",
    "1. **Masked Multi-head self-attention**\n",
    "   ‚Üí Each position can only attend to previous tokens (to prevent cheating during generation).\n",
    "\n",
    "2. **Encoder-Decoder attention**\n",
    "   ‚Üí The decoder attends to encoder outputs ‚Äî this is how the decoder knows what the input meant.\n",
    "\n",
    "3. **Feedforward network**\n",
    "   ‚Üí Like in the encoder, applies transformation to each position.\n",
    "\n",
    "Each layer builds a **richer representation** of the output sequence being generated.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Final Step: Linear + Softmax\n",
    "\n",
    "* After decoder layers, the final output goes through a **linear layer** followed by **softmax** to predict the next word.\n",
    "* This output is a probability distribution over the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Example: English to French Translation\n",
    "\n",
    "```text\n",
    "Input: \"I love transformers\"\n",
    "‚Üì\n",
    "Encoder processes this and creates contextual embeddings\n",
    "‚Üì\n",
    "Decoder begins with: \"<start>\" token\n",
    "‚Üì\n",
    "Decoder predicts \"J'\"\n",
    "‚Üì\n",
    "Then uses \"J'\" + context to predict \"aime\"\n",
    "‚Üì\n",
    "Repeats until \"<end>\" is generated\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Config Summary (from diagram):\n",
    "\n",
    "* `Encoder Layers: 6`\n",
    "* `Decoder Layers: 6`\n",
    "* `Heads: 8` (each layer has 8 attention heads)\n",
    "* `d_model: 512` (embedding size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29001/29001 [00:00<00:00, 36164.77it/s]\n",
      "Building vocab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29001/29001 [00:01<00:00, 23170.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab Size English 10837\n",
      "\n",
      "Vocab Size German 19214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import Multi30k\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def ensure_spacy_model(model_name):\n",
    "    try:\n",
    "        spacy.load(model_name)\n",
    "    except OSError:\n",
    "        print(f\"Downloading spaCy model '{model_name}'...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "\n",
    "# Ensure required spaCy models are installed\n",
    "ensure_spacy_model(\"en_core_web_sm\")\n",
    "ensure_spacy_model(\"de_core_news_sm\")\n",
    "\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return en_tokenizer(str(text))\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return de_tokenizer(str(text))\n",
    "\n",
    "# --- FIXED DATA LOADING ---\n",
    "train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
    "train_data_en = []\n",
    "train_data_de = []\n",
    "for en, de in train_iter:\n",
    "    train_data_en.append(en)\n",
    "    train_data_de.append(de)\n",
    "# --- END FIX ---\n",
    "\n",
    "class VOCAB:\n",
    "    def __init__(self, tokenizer, min_freq=2, data=None, special_tokens=['<pad>', '<sos>', '<eos>', '<unk>']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = special_tokens\n",
    "        self.build_vocab(data)\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        counter = Counter()\n",
    "        for text in tqdm(data, desc=\"Building vocab\"):\n",
    "            tokens = self.tokenizer(text)\n",
    "            counter.update(tokens)\n",
    "        tokens = [token for token, freq in counter.items() if freq >= self.min_freq and token not in self.special_tokens]\n",
    "        tokens = self.special_tokens + tokens\n",
    "        self.stoi = {token: index for index, token in enumerate(tokens)}\n",
    "        self.itos = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi['<unk>'])\n",
    "\n",
    "EN_VOCAB = VOCAB(tokenize_en, min_freq=1, data=train_data_en)\n",
    "DE_VOCAB = VOCAB(tokenize_de, min_freq=1, data=train_data_de)\n",
    "print(\"\\nVocab Size English\", len(EN_VOCAB))\n",
    "print(\"\\nVocab Size German\", len(DE_VOCAB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Data Ingestion & Vocabulary Creation for Machine Translation\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "\n",
    "In this cell, I implemented the **data ingestion pipeline and vocabulary builder** for a machine translation task using the **Multi30k dataset** from `torchtext`. Specifically:\n",
    "\n",
    "1. Downloaded and verified the required **spaCy tokenizers** for English (`en_core_web_sm`) and German (`de_core_news_sm`).\n",
    "2. Defined **language-specific tokenizers** to split sentences into words/tokens.\n",
    "3. Loaded the **training split** of the Multi30k dataset, extracting parallel English-German sentence pairs.\n",
    "4. Built a custom `VOCAB` class that:\n",
    "   - Tokenizes each sentence,\n",
    "   - Counts token frequencies,\n",
    "   - Filters out rare tokens (based on `min_freq`),\n",
    "   - Stores mappings from token to index (`stoi`) and index to token (`itos`).\n",
    "5. Constructed vocabularies for both English and German data.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind This Cell\n",
    "\n",
    "Before feeding sentences into a Transformer, we must convert words into **numerical indices** that the model can process. However, raw text is unstructured ‚Äî every model needs a **vocabulary** to map each unique token to an integer index.\n",
    "\n",
    "This step is **crucial** in NLP workflows because:\n",
    "- It determines the **size and content of the embedding space**.\n",
    "- It defines how **unknown and special tokens** (`<pad>`, `<sos>`, `<eos>`, `<unk>`) are handled.\n",
    "- A good vocabulary reduces overfitting by filtering out noisy/rare words.\n",
    "\n",
    "Tokenization also ensures that input sentences are split **accurately and consistently**, especially across languages like English and German.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Step-by-Step\n",
    "\n",
    "1. **spaCy Model Verification**:\n",
    "   - Calls `ensure_spacy_model()` to check if the tokenizer models are available.\n",
    "   - Downloads them if missing.\n",
    "\n",
    "2. **Tokenizers**:\n",
    "   - `tokenize_en()` and `tokenize_de()` wrap the spaCy tokenizers for easy access.\n",
    "\n",
    "3. **Data Loading**:\n",
    "   - Uses `torchtext.datasets.Multi30k` to load English-German sentence pairs from the training set.\n",
    "   - Stores them in separate lists: `train_data_en` and `train_data_de`.\n",
    "\n",
    "4. **Vocabulary Building**:\n",
    "   - The `VOCAB` class tokenizes each sentence, counts token frequencies, and keeps only those above `min_freq`.\n",
    "   - Initializes the mapping of tokens to indices (`stoi`) and vice versa (`itos`), including special tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "\n",
    "- **Token-to-index mapping is foundational** for all NLP models.\n",
    "- Ensures **consistent preprocessing** for training, validation, and inference.\n",
    "- **Handles unknowns, padding, and sequence boundaries** in a standardized way using special tokens.\n",
    "- Enables **efficient batch processing** and embedding lookup via integer indexing.\n",
    "- Filtering rare tokens reduces vocabulary size, which leads to **faster training and better generalization**.\n",
    "\n",
    "Without this step, the Transformer would not understand what to embed, attend to, or decode.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When Do We Use This?\n",
    "\n",
    "This step is used:\n",
    "- **Before training** a sequence-to-sequence model like a Transformer.\n",
    "- **During inference** to preprocess input and decode output.\n",
    "- Whenever working with **custom or external datasets** where tokenization and vocabulary building are not pre-defined.\n",
    "- In **low-resource** or **domain-specific** translation settings where vocabularies need to be tailored manually.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This cell sets up the entire **preprocessing foundation** required for training a machine translation model. From downloading language tokenizers to building numerical vocabularies, this is a critical first step to enable deep learning models to understand and process language effectively. üåçüî§‚û°Ô∏èüìä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code prepares bilingual text data (English ‚Üî German) for a machine translation model using the **Multi30k dataset**.\n",
    "\n",
    "Here's what it does:\n",
    "\n",
    "* ‚úÖ **Loads English‚ÄìGerman sentence pairs** using `torchtext`.\n",
    "* üß† **Tokenizes** the sentences using **spaCy**, a language-aware tokenizer that handles grammar, punctuation, and morphology.\n",
    "* üì¶ **Builds vocabularies** for both languages:\n",
    "\n",
    "  * Assigns each token a unique index.\n",
    "  * Includes special tokens like `<pad>`, `<sos>`, `<eos>`, and `<unk>`.\n",
    "  * Filters out rare words (optional via `min_freq`).\n",
    "* üî¢ The final output is a mapping from words ‚Üí numbers, which is essential for training neural networks.\n",
    "\n",
    "This setup forms the **first step in building a translation model**‚Äîconverting raw text into something a neural model can understand and learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, en_data, de_data, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with source (English) and target (German) data,\n",
    "        along with their respective tokenizers and vocabularies.\n",
    "        \"\"\"\n",
    "        self.en_data = en_data  # List of English sentences\n",
    "        self.de_data = de_data  # List of German sentences\n",
    "        self.src_tokenizer = src_tokenizer  # Tokenizer for English\n",
    "        self.tgt_tokenizer = tgt_tokenizer  # Tokenizer for German\n",
    "        self.src_vocab = src_vocab  # English vocabulary (token -> index)\n",
    "        self.tgt_vocab = tgt_vocab  # German vocabulary (token -> index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Process a single (English, German) sentence pair:\n",
    "        - Tokenize\n",
    "        - Convert tokens to indices\n",
    "        - Add <sos> and <eos> special tokens\n",
    "        - Return tensors for both source and target\n",
    "        \"\"\"\n",
    "        src_txt, tgt_txt = self.en_data[index], self.de_data[index]\n",
    "\n",
    "        # Tokenize and convert to indices using vocab\n",
    "        src_tokens = [self.src_vocab[token] for token in self.src_tokenizer(src_txt)]\n",
    "        tgt_tokens = [self.tgt_vocab[token] for token in self.tgt_tokenizer(tgt_txt)]\n",
    "\n",
    "        # Add <sos> and <eos> tokens around the sequences\n",
    "        src_tokens = [self.src_vocab['<sos>']] + src_tokens + [self.src_vocab['<eos>']]\n",
    "        tgt_tokens = [self.tgt_vocab['<sos>']] + tgt_tokens + [self.tgt_vocab['<eos>']]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        src_tensor = torch.LongTensor(src_tokens)\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens)\n",
    "\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of sentence pairs in the dataset.\n",
    "        \"\"\"\n",
    "        assert len(self.en_data) == len(self.de_data)  # Ensure aligned data\n",
    "        return len(self.en_data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom function to pad batches of variable-length sequences:\n",
    "        - Pads all source and target tensors in the batch to the same length\n",
    "        - Uses <pad> token index from vocab\n",
    "        - Returns two padded tensors (src_batch, tgt_batch)\n",
    "        \"\"\"\n",
    "        src_tensors, tgt_tensors = zip(*batch)  # Unzip list of (src, tgt) pairs\n",
    "\n",
    "        # Pad sequences to match longest in batch (for batching)\n",
    "        src_tensors = torch.nn.utils.rnn.pad_sequence(\n",
    "            src_tensors, padding_value=self.src_vocab['<pad>'], batch_first=True\n",
    "        )\n",
    "        tgt_tensors = torch.nn.utils.rnn.pad_sequence(\n",
    "            tgt_tensors, padding_value=self.tgt_vocab['<pad>'], batch_first=True\n",
    "        )\n",
    "\n",
    "        return src_tensors, tgt_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ TranslationDataset: Custom Dataset for Sequence-to-Sequence Learning\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "\n",
    "In this cell, I implemented a **custom PyTorch Dataset class** `TranslationDataset` to preprocess and prepare English-German sentence pairs for training a Transformer model. The class:\n",
    "\n",
    "- Tokenizes each sentence using spaCy tokenizers.\n",
    "- Converts tokens to vocabulary indices.\n",
    "- Adds special tokens (`<sos>`, `<eos>`) for marking sentence start and end.\n",
    "- Pads variable-length sequences to support efficient batching using a custom `collate_fn`.\n",
    "\n",
    "This forms a fully functional data pipeline for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind the Dataset Class\n",
    "\n",
    "Deep learning models like Transformers require:\n",
    "1. **Tensors as inputs**, not raw text.\n",
    "2. **Fixed-length batches**, which means dynamic padding is needed.\n",
    "3. **Start and end markers** in sequence generation tasks (e.g., translation, summarization).\n",
    "\n",
    "This class wraps those requirements into a clean, modular interface compatible with PyTorch‚Äôs `DataLoader`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Step-by-Step\n",
    "\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - Accepts English and German sentence lists.\n",
    "   - Takes corresponding tokenizers and vocabularies for both languages.\n",
    "   - Stores them for use in `__getitem__`.\n",
    "\n",
    "2. **Single Example Processing (`__getitem__`)**:\n",
    "   - Selects one English-German sentence pair by index.\n",
    "   - Tokenizes both using their respective tokenizers.\n",
    "   - Converts tokens to indices using the `VOCAB` class.\n",
    "   - Adds `<sos>` (start of sequence) and `<eos>` (end of sequence) tokens to help the model learn generation boundaries.\n",
    "   - Returns tensors for both source and target sequences.\n",
    "\n",
    "3. **Dataset Size (`__len__`)**:\n",
    "   - Returns the number of sentence pairs in the dataset (used by `DataLoader` for batching).\n",
    "\n",
    "4. **Batch Padding (`collate_fn`)**:\n",
    "   - Custom function for `DataLoader` to batch variable-length sequences.\n",
    "   - Uses `torch.nn.utils.rnn.pad_sequence` to pad all sequences in a batch to the maximum length in that batch.\n",
    "   - Pads with the `<pad>` token index so the model can later ignore those positions.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This is Important\n",
    "\n",
    "- Ensures **consistent preprocessing** across every sample during training.\n",
    "- Enables **efficient batch processing** using PyTorch‚Äôs `DataLoader` (essential for GPU training).\n",
    "- Handles **padding and token alignment** correctly ‚Äî a common source of bugs in seq2seq models.\n",
    "- Easily integrates with attention-based models that require sequence lengths to be uniform within a batch.\n",
    "\n",
    "Without this, it would be much harder to:\n",
    "- Train on large-scale data efficiently.\n",
    "- Ensure correct behavior of masking and alignment in Transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When to Use This\n",
    "\n",
    "Use this custom dataset when:\n",
    "- You are working on **sequence-to-sequence tasks** (like translation, summarization, or dialogue generation).\n",
    "- Your sequences vary in length and need **dynamic padding**.\n",
    "- You want to integrate tokenization, index conversion, and special token management in one place.\n",
    "- You plan to use a PyTorch `DataLoader` for training your model in batches.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This `TranslationDataset` is a critical bridge between raw data and model input. It handles everything from tokenization and indexing to padding and batching, enabling a robust and clean training pipeline for NLP tasks using PyTorch. ‚öôÔ∏èüìöüöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct Multi30k data folder and file names as per the multi30k_data folder and the 2016 split\n",
    "\n",
    "import gzip\n",
    "\n",
    "def read_gzipped_lines(filepath):\n",
    "    with gzip.open(filepath, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# Paths to gzipped files in the multi30k_data folder\n",
    "train_en_file = \"multi30k/train.en.gz\"\n",
    "train_de_file = \"multi30k/train.de.gz\"\n",
    "val_en_file   = \"multi30k/val.en.gz\"\n",
    "val_de_file   = \"multi30k/val.de.gz\"\n",
    "test_en_file  = \"multi30k/test_2016_flickr.en.gz\"\n",
    "test_de_file  = \"multi30k/test_2016_flickr.de.gz\"\n",
    "\n",
    "# Read the data from gzipped files\n",
    "train_data_en = read_gzipped_lines(train_en_file)\n",
    "train_data_de = read_gzipped_lines(train_de_file)\n",
    "val_data_en   = read_gzipped_lines(val_en_file)\n",
    "val_data_de   = read_gzipped_lines(val_de_file)\n",
    "test_data_en  = read_gzipped_lines(test_en_file)\n",
    "test_data_de  = read_gzipped_lines(test_de_file)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data_en, train_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "val_dataset = TranslationDataset(val_data_en, val_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "test_dataset = TranslationDataset(test_data_en, test_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Loading Preprocessed Multi30k Data & Creating DataLoaders\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "\n",
    "In this cell, I:\n",
    "1. Defined a utility function to read **gzipped files** line by line (as text).\n",
    "2. Loaded the **English-German sentence pairs** from the `Multi30k` dataset's gzipped files:\n",
    "   - For **training**, **validation**, and **test** sets (2016 Flickr split).\n",
    "3. Wrapped these sentence pairs into `TranslationDataset` instances.\n",
    "4. Created corresponding **PyTorch `DataLoader`s** to:\n",
    "   - Efficiently batch and shuffle training data\n",
    "   - Prepare padded batches for the Transformer model\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind This Pipeline\n",
    "\n",
    "- The `Multi30k` dataset is stored as compressed `.gz` text files. We must **manually extract** and preprocess this raw text.\n",
    "- `TranslationDataset` handles **tokenization, numerical encoding, and padding**, ensuring the data is ready for the model.\n",
    "- `DataLoader` enables:\n",
    "  - **Batching** for faster training\n",
    "  - **Shuffling** for training robustness\n",
    "  - **Custom collation** to dynamically pad sequences to the longest sentence in each batch\n",
    "\n",
    "This setup ensures a smooth, repeatable pipeline from raw `.gz` files to ready-to-train batches.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works Step-by-Step\n",
    "\n",
    "1. **Reading Compressed Files**:\n",
    "   - `read_gzipped_lines(filepath)` uses Python‚Äôs `gzip` module to read `.gz` files line by line.\n",
    "   - Each line is stripped of whitespace to prepare a clean sentence list.\n",
    "\n",
    "2. **Loading Multi30k 2016 Data**:\n",
    "   - Trains with `train.en.gz` and `train.de.gz`\n",
    "   - Validates with `val.en.gz` and `val.de.gz`\n",
    "   - Tests with `test_2016_flickr.en.gz` and `test_2016_flickr.de.gz`\n",
    "   - Each file contains **sentence-aligned** English and German data.\n",
    "\n",
    "3. **Wrapping with `TranslationDataset`**:\n",
    "   - Handles tokenization, vocabulary lookup, and special token addition (`<sos>`, `<eos>`).\n",
    "   - Returns padded tensors via `collate_fn`.\n",
    "\n",
    "4. **Creating PyTorch DataLoaders**:\n",
    "   - Batches of size `BATCH_SIZE = 128` are created.\n",
    "   - `shuffle=True` ensures randomness during training.\n",
    "   - Padding is handled dynamically in `collate_fn` so sequences are model-ready.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why This Is Important\n",
    "\n",
    "- Most real-world datasets are **compressed** or in non-PyTorch-native formats ‚Äî this step shows how to bridge that.\n",
    "- Batching and padding are **essential for GPU efficiency and stability** in training.\n",
    "- Ensures **consistency and reproducibility** in data processing across training, validation, and testing.\n",
    "- Helps in **avoiding OOM errors** and ensures that shorter sequences don't affect model convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When to Use This\n",
    "\n",
    "Use this setup when:\n",
    "- You are working with **custom or raw parallel corpora** in `.txt.gz` or `.csv.gz` formats.\n",
    "- You want to build a **training pipeline compatible with Transformers or any seq2seq model**.\n",
    "- You need **fast iteration, batching, and GPU-optimized t**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  2]),\n",
       " tensor([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> Two young , White males are outside near many bushes . <eos>',\n",
       " '<sos> Zwei junge wei√üe M√§nner sind im Freien in der N√§he vieler B√ºsche . <eos>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN_VOCAB.itos[i] for i in train_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in train_dataset[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A man in an orange hat starring at something.',\n",
       " 'Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_en[0], test_data_de[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> A man in an orange hat starring at something . <eos>',\n",
       " '<sos> Ein Mann mit einem orangefarbenen Hut , der etwas <unk> . <eos>')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN_VOCAB.itos[i] for i in test_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in test_dataset[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and associated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hyperparameters\n",
    "# Define Hyper Parameters\n",
    "NUM_EPOCHS      = 20\n",
    "D_MODEL         = 256\n",
    "ATTN_HEADS      = 8\n",
    "NUM_LAYERS      = 3\n",
    "FEEDFORWARD_DIM = 512\n",
    "DROPOUT         = 0.1\n",
    "MAX_SEQ_LEN     = 150\n",
    "SRC_VOCAB_SIZE  = len(EN_VOCAB)\n",
    "TGT_VOCAB_SIZE  = len(DE_VOCAB)\n",
    "LR              = 0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Defining Hyperparameters for the Transformer Model\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "\n",
    "In this cell, I defined the key **hyperparameters** required to build, train, and evaluate the Transformer model for machine translation (English to German). These parameters control the model‚Äôs size, training duration, regularization, and computation settings.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition Behind Each Hyperparameter\n",
    "\n",
    "- **NUM_EPOCHS = 20**  \n",
    "  Number of full passes over the training dataset. A higher number allows better convergence but risks overfitting if too large.\n",
    "\n",
    "- **D_MODEL = 256**  \n",
    "  The size of each token embedding and the hidden dimension in the Transformer. This controls the model's capacity ‚Äî a higher value means more expressive power but also more compute.\n",
    "\n",
    "- **ATTN_HEADS = 8**  \n",
    "  Number of parallel attention heads in the multi-head self-attention mechanism. Each head learns different relational patterns in the sequence.\n",
    "\n",
    "- **NUM_LAYERS = 3**  \n",
    "  Number of encoder and decoder layers (i.e., Transformer blocks). More layers improve modeling depth but increase training time and memory use.\n",
    "\n",
    "- **FEEDFORWARD_DIM = 512**  \n",
    "  Size of the intermediate layer in the position-wise feedforward network. Typically set to 2‚Äì4√ó `D_MODEL`.\n",
    "\n",
    "- **DROPOUT = 0.1**  \n",
    "  Dropout probability used for regularization to prevent overfitting. Applied after attention and feedforward layers.\n",
    "\n",
    "- **MAX_SEQ_LEN = 150**  \n",
    "  Maximum sequence length expected for both source and target. Used to construct the positional encoding matrix.\n",
    "\n",
    "- **SRC_VOCAB_SIZE = len(EN_VOCAB)**  \n",
    "  Size of the input vocabulary (English). Needed to initialize the embedding layer for the encoder.\n",
    "\n",
    "- **TGT_VOCAB_SIZE = len(DE_VOCAB)**  \n",
    "  Size of the output vocabulary (German). Needed to initialize the embedding and final projection layer for the decoder.\n",
    "\n",
    "- **LR = 0**  \n",
    "  Placeholder for learning rate (will typically be defined later, e.g., with warmup scheduling or Adam optimizer setup).\n",
    "\n",
    "- **DEVICE = torch.device(...)**  \n",
    "  Automatically sets the computation device to **GPU (if available)** or **CPU**, ensuring training compatibility across environments.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Why These Are Important\n",
    "\n",
    "- These parameters directly affect **model performance**, **training time**, and **resource usage**.\n",
    "- They allow easy experimentation and tuning without modifying the model architecture.\n",
    "- Keeping hyperparameters centralized makes the project more **maintainable and reproducible**.\n",
    "\n",
    "---\n",
    "\n",
    "### üï∞Ô∏è When to Modify These\n",
    "\n",
    "Adjust these hyperparameters:\n",
    "- When switching to a **larger or smaller dataset**\n",
    "- To reduce **memory usage or training time**\n",
    "- To improve **model accuracy or convergence**\n",
    "- During **hyperparameter tuning** or **grid/random search** for optimal configuration\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "This cell defines the foundational settings that control the behavior and training of the Transformer model. They balance expressiveness, training stability, and compute efficiency ‚Äî and can be tuned based on the problem scale and hardware availability. üß†‚öôÔ∏èüöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class NoamScheduler:\n",
    "    def __init__(self,optimizer,d_model, warmup_steps = 4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        step = self.current_step\n",
    "        # Add a small epsilon to avoid division by zero if step is 0\n",
    "        return (self.d_model ** -0.5) * min((step + 1e-9) ** -0.5, step * self.warmup_steps ** -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, ATTN_HEADS, FEEDFORWARD_DIM, MAX_SEQ_LEN, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "# optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9, weight_decay=5e-2)\n",
    "warmup_steps = 2 * len(train_dataloader)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda step: (D_MODEL ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5), verbose=True)\n",
    "scheduler = NoamScheduler(optimizer, d_model=D_MODEL, warmup_steps=warmup_steps)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=DE_VOCAB['<pad>'], label_smoothing=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Model Initialization, Optimizer, Scheduler, Loss & Precision Setup\n",
    "\n",
    "### üß† What I Did in This Cell\n",
    "\n",
    "In this step, I completed the full training configuration for the Transformer-based machine translation model. This includes:\n",
    "1. Initializing the Transformer model with vocabulary sizes and architecture.\n",
    "2. Defining the `AdamW` optimizer with proper settings for Transformers.\n",
    "3. Setting up a custom learning rate scheduler (`NoamScheduler`) as described in the original paper.\n",
    "4. Defining a smoothed cross-entropy loss function with padding ignored.\n",
    "5. Enabling automatic mixed-precision (AMP) training for efficient GPU usage.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Breakdown & Intuition\n",
    "\n",
    "#### 1. **Model Initialization**\n",
    "```python\n",
    "model = Transformer(...).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def generate_tgt_mask(tgt, pad_idx):\n",
    "    seq_len = tgt.size(1)\n",
    "    no_future_mask = torch.tril(torch.ones((seq_len, seq_len), device=DEVICE)).bool()\n",
    "    pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    combined_mask = pad_mask & no_future_mask\n",
    "    return combined_mask\n",
    "\n",
    "def generate_src_mask(src, pad_idx):\n",
    "    mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return mask\n",
    "\n",
    "def calculate_bleu(tgt_output, output):\n",
    "    tgt_output = tgt_output.cpu().numpy()\n",
    "    output = output.cpu().numpy()\n",
    "\n",
    "    refs = []\n",
    "    hyps = []\n",
    "\n",
    "    for tgt, pred in zip(tgt_output, output):\n",
    "        ref = ' '.join([DE_VOCAB.itos[t] for t in tgt if t not in (DE_VOCAB['<pad>'], DE_VOCAB['<eos>'], DE_VOCAB['<sos>'])])\n",
    "        hyp = ' '.join([DE_VOCAB.itos[t] for t in pred if t not in (DE_VOCAB['<pad>'], DE_VOCAB['<eos>'], DE_VOCAB['<sos>'])])\n",
    "\n",
    "        refs.append(ref)\n",
    "        hyps.append(hyp)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs], force=True).score\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Evaluation Utilities: BLEU Score, Masks, and Setup\n",
    "\n",
    "### üß† What This Cell Does\n",
    "\n",
    "This cell prepares the essential components required for **evaluating the performance** of the Transformer model during inference, especially using BLEU score ‚Äî a popular metric in machine translation.\n",
    "\n",
    "It includes:\n",
    "1. Library imports and installation for `nltk` and `sacrebleu`.\n",
    "2. Functions to generate appropriate **masks** for source and target sequences.\n",
    "3. A function to **calculate the BLEU score** based on model predictions and true target sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ 1. Import & Install Evaluation Libraries\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    subprocess.check_call([...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # print(\"Src\", src.shape, \"Tgt\", tgt.shape)\n",
    "        src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            loss = criterion(output.reshape(-1, output.size(2)), tgt_output.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(total_loss / (i + 1)),\n",
    "            lr=\"{:.09f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, DEVICE):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_bleu_score = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Validate')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "            tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "                loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bleu_score += calculate_bleu(tgt_output, output.argmax(-1))\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(epoch_loss / (i + 1)),\n",
    "            bleu=\"{:.04f}\".format(epoch_bleu_score / (i + 1)))\n",
    "\n",
    "            batch_bar.update()\n",
    "\n",
    "    # Normalize the loss and BLEU score by the number of validation samples\n",
    "    epoch_loss /= len(dataloader)\n",
    "    epoch_bleu_score /= len(dataloader)\n",
    "\n",
    "    return epoch_loss, epoch_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Epoch Training & Validation Loops\n",
    "\n",
    "### üß† What This Cell Does\n",
    "\n",
    "This cell defines two critical functions for the training workflow:\n",
    "- `train_epoch(...)`: Trains the model for **one full pass** over the training data.\n",
    "- `validate_epoch(...)`: Evaluates the model on validation data after each epoch.\n",
    "\n",
    "Together, these functions handle:\n",
    "- Mask generation\n",
    "- Forward and backward passes\n",
    "- Optimizer and scheduler updates\n",
    "- Loss tracking\n",
    "- BLEU score evaluation (for translation quality)\n",
    "\n",
    "---\n",
    "\n",
    "### üîÇ `train_epoch(...)` ‚Äî Function Overview\n",
    "\n",
    "#### ‚úÖ Function Purpose\n",
    "- Trains the Transformer model on each batch of the training data.\n",
    "- Computes loss using teacher forcing.\n",
    "- Applies mixed-precision training with gradient scaling.\n",
    "- Updates model parameters and learning rate.\n",
    "\n",
    "#### üîß Key Steps\n",
    "\n",
    "1. **Set model to training mode**\n",
    "```python\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, src, de_tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "\n",
    "    # Initialize target input tensors with <sos> tokens\n",
    "    tgt_input = torch.full((src.size(0), 1), DE_VOCAB['<sos>'], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # Create a flag for each sequence in the batch\n",
    "    eos_flags = torch.zeros(src.size(0), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    # Perform inference for each target token\n",
    "    with torch.no_grad():\n",
    "        for _ in range(70):\n",
    "            tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            next_tokens = output.argmax(2)[:, -1].unsqueeze(1)\n",
    "            tgt_input = torch.cat((tgt_input, next_tokens), dim=1)\n",
    "\n",
    "            # Update the eos_flags for sequences that have generated <eos>\n",
    "            eos_flags |= (next_tokens.squeeze() == DE_VOCAB['<eos>'])\n",
    "\n",
    "            # Stop generating tokens if all sequences have generated <eos> or reached maximum length\n",
    "            if torch.all(eos_flags):\n",
    "                break\n",
    "\n",
    "    # Convert target input tensors to translated sentences\n",
    "    translated_sentences = []\n",
    "    for i in range(tgt_input.size(0)):\n",
    "        translated_tokens = []\n",
    "        for token in tgt_input[i][1:]:\n",
    "            if token == DE_VOCAB['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                translated_tokens.append(DE_VOCAB.itos[token.item()])\n",
    "        translated_sentence = ' '.join(translated_tokens)\n",
    "        translated_sentences.append(translated_sentence)\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Inference: Translating New Sentences with the Transformer Model\n",
    "\n",
    "### üß† What This Function Does\n",
    "\n",
    "The `inference(...)` function performs **greedy decoding** using a trained Transformer model to generate target (German) sentences from source (English) input sequences. It generates tokens one by one, stopping when the `<eos>` (end-of-sequence) token is produced or a maximum length is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Key Steps and Intuition\n",
    "\n",
    "1. **Set Model to Evaluation Mode**\n",
    "```python\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embedding): Embedding(10837, 256)\n",
      "  (tgt_embedding): Embedding(19214, 256)\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-2): 3 x EncoderBlock(\n",
      "      (self_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm2): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-2): 3 x DecoderBlock(\n",
      "      (self_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (enc_dec_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm2): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm3): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=19214, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8258 | Val Loss: 4.2103 | BLEU Score: 6.6498\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8228 | Val Loss: 3.6152 | BLEU Score: 10.5960\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3153 | Val Loss: 3.3305 | BLEU Score: 14.9178\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9369 | Val Loss: 3.1557 | BLEU Score: 14.2710\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6769 | Val Loss: 3.0911 | BLEU Score: 16.9041\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4787 | Val Loss: 3.0350 | BLEU Score: 19.7898\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3186 | Val Loss: 3.0254 | BLEU Score: 20.5710\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1906 | Val Loss: 2.9974 | BLEU Score: 27.9643\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0868 | Val Loss: 3.0169 | BLEU Score: 19.5570\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0041 | Val Loss: 3.0241 | BLEU Score: 19.3396\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9401 | Val Loss: 3.0485 | BLEU Score: 19.6496\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8880 | Val Loss: 3.0643 | BLEU Score: 20.1703\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8431 | Val Loss: 3.0741 | BLEU Score: 20.1040\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8054 | Val Loss: 3.1011 | BLEU Score: 19.0616\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7747 | Val Loss: 3.1071 | BLEU Score: 17.7373\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7470 | Val Loss: 3.1435 | BLEU Score: 18.8883\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7211 | Val Loss: 3.1534 | BLEU Score: 18.3990\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7001 | Val Loss: 3.1793 | BLEU Score: 23.1467\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6813 | Val Loss: 3.1750 | BLEU Score: 28.2455\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6653 | Val Loss: 3.1796 | BLEU Score: 23.5873\n",
      "\n",
      "Training complete. Best validation loss: 2.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Make sure the following are defined before running:\n",
    "# NUM_EPOCHS, model, optimizer, criterion, DEVICE, scheduler (optional)\n",
    "# train_epoch, validate_epoch, train_dataloader, val_dataloader\n",
    "\n",
    "def main():\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # ----- Training -----\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        val_loss, bleu_score = validate_epoch(model, val_dataloader, criterion, DEVICE)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # ----- Print epoch summary -----\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "        # ----- Save the best model -----\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"‚úÖ Best model saved.\")\n",
    "\n",
    "        # ----- Step the scheduler if available -----\n",
    "        if 'scheduler' in locals() and scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    # Save loss/metric history\n",
    "    np.save(\"train_losses.npy\", np.array(train_losses))\n",
    "    np.save(\"val_losses.npy\", np.array(val_losses))\n",
    "    np.save(\"bleu_scores.npy\", np.array(bleu_scores))\n",
    "    print(f\"\\nTraining complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Main Training Loop\n",
    "\n",
    "### üß† What This Code Does\n",
    "\n",
    "This `main()` function runs the **full training pipeline** for the Transformer model across multiple epochs. It orchestrates the training and validation workflow, tracks performance metrics (loss & BLEU score), saves the best-performing model, and records history for later analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Key Functionality Explained\n",
    "\n",
    "#### üîÅ Epoch Loop\n",
    "```python\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:50<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example Sentence and its Translation\n",
      "Source Sentence in English               : A female performer with a violin plays on a street while a woman with a blue guitar looks on .\n",
      "German have the truth          : Eine <unk> mit einer Violine spielt auf der Stra√üe w√§hrend eine Frau mit einer blauen Gitarre zusieht .\n",
      "Machine Translated Sentence in German    : Eine K√ºnstlerin spielt auf einer Stra√üe mit einer Violine , w√§hrend eine Frau mit einer blauen Gitarre zuschaut .\n",
      "Test BLEU score: 54.91004867761124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def evaluate_test_set_bleu(model, test_dataloader, de_tokenizer):\n",
    "    translated_sentences = []\n",
    "    ground_truth_sentences = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        src, tgt_output = batch\n",
    "        src, tgt = src.to(DEVICE), tgt_output.to(DEVICE)\n",
    "        tgt_sentences = [' '.join([DE_VOCAB.itos[token.item()] for token in sequence if token.item() not in [DE_VOCAB['<pad>'], DE_VOCAB['<sos>'], DE_VOCAB['<eos>']]]) for sequence in tgt_output]\n",
    "\n",
    "        translations = inference(model, src, de_tokenizer)\n",
    "        translated_sentences.extend(translations)\n",
    "        ground_truth_sentences.extend([[tgt] for tgt in tgt_sentences])\n",
    "\n",
    "    rand_index = random.randint(0, len(test_dataset))\n",
    "    print(\"\\n\\nExample Sentence and its Translation\")\n",
    "    print(\"Source Sentence in English               :\", ' '.join([EN_VOCAB.itos[i] for i in test_dataset[rand_index][0] if EN_VOCAB.itos[i] not in ['<pad>', '<sos>', '<eos>']]))\n",
    "    print(\"German have the truth          :\", ground_truth_sentences[rand_index][0])\n",
    "    print(\"Machine Translated Sentence in German    :\", translated_sentences[rand_index])\n",
    "    bleu_score = sacrebleu.corpus_bleu(translated_sentences, ground_truth_sentences)\n",
    "    return bleu_score\n",
    "\n",
    "# Usage example\n",
    "test_bleu = evaluate_test_set_bleu(model, test_dataloader, de_tokenizer)\n",
    "print(\"Test BLEU score:\", test_bleu.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Set Evaluation with BLEU Score\n",
    "\n",
    "### üéØ Purpose\n",
    "\n",
    "This function evaluates the **final performance** of the trained Transformer model on the test dataset using the **BLEU (Bilingual Evaluation Understudy)** score. It provides:\n",
    "- Quantitative metric: BLEU score\n",
    "- Qualitative insights: A randomly selected example of a source sentence, its ground truth translation, and the model‚Äôs translation\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Happens in This Function?\n",
    "\n",
    "#### 1. **Initialize Storage**\n",
    "```python\n",
    "translated_sentences = []\n",
    "ground_truth_sentences = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
