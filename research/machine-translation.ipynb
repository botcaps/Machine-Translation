{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/anaconda3/envs/machine_translation/lib/python3.10/site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#  %pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.0\n",
      "  Using cached numpy-2.0.0-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.0.0-cp310-cp310-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install numpy==2.0\n",
    "# %pip install \"numpy<2.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                                # PyTorch main package for tensor operations and deep learning\n",
    "import torch.nn as nn                                       # PyTorch module for building neural network layers\n",
    "import torch.optim as optim                                 # PyTorch module for optimization algorithms (e.g., Adam, SGD)\n",
    "import sacrebleu                                            # Library for calculating BLEU score (translation quality metric)\n",
    "from torchtext.data.utils import get_tokenizer              # Utility to get tokenizers for text preprocessing\n",
    "from torchtext.vocab import build_vocab_from_iterator       # Function to build vocabulary from tokenized data\n",
    "from torchtext.datasets import Multi30k                     # Multi30k dataset for English-German/French translation tasks\n",
    "from typing import Tuple                                    # Type hinting for functions that return tuples\n",
    "import spacy                                                # NLP library for tokenization and linguistic features\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)                #check the version of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadSelfAttention module.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): Total dimension of the model.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Ensure the model dimension is divisible by number of heads\n",
    "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Linear layers to project input into Q, K, V\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Final linear layer after concatenating attention output from all heads\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for MultiHeadSelfAttention.\n",
    "\n",
    "        Args:\n",
    "        q (Tensor): Query tensor of shape (batch_size, seq_length, d_model).\n",
    "        k (Tensor): Key tensor of shape (batch_size, seq_length, d_model).\n",
    "        v (Tensor): Value tensor of shape (batch_size, seq_length, d_model).\n",
    "        mask (Tensor, optional): Mask tensor. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Apply linear layers and split into multiple heads\n",
    "        # Output shape: (batch_size, num_heads, seq_length, head_dim)\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # shape: (batch_size, num_heads, seq_length, seq_length)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Apply mask (e.g., for causal or padding attention)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # Normalize over keys\n",
    "        attn_output = torch.matmul(attn_weights, v)    # shape: (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.wo(attn_output)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self-attention splits the input into multiple attention \"heads\" to learn different patterns.\n",
    "### Queries, Keys, and Values are projected from the same input using linear layers.\n",
    "### Scaled dot-product attention is applied to compute weighted representations.\n",
    "### All heads are concatenated and passed through a final linear projection to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position- wise Feed Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the PositionwiseFeedForward module.\n",
    "        \n",
    "        Args:\n",
    "        d_model (int): The dimensionality of the input.\n",
    "        d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
    "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Forward pass for PositionwiseFeedForward.\n",
    "        \n",
    "        Args:\n",
    "        x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.linear1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps =1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the AddNorm module.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): The dimensionality of the input.\n",
    "        eps (float, optional): A small constant for numerical stability. Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model,eps = eps)\n",
    "\n",
    "    \n",
    "    def forward(self,x,residual):\n",
    "        \"\"\"\n",
    "        Forward pass for AddNorm.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "        residual (Tensor): The residual tensor of the same shape as the input tensor.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        out = x +residual\n",
    "        out = self.norm(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): The dimensionality of the input.\n",
    "        max_seq_len (int): The maximum length of the input sequence.\n",
    "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model,2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for PositionalEncoding.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output tensor of shape (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "\n",
    "The Encoder Block in the Transformer architecture consists of the following layers:\n",
    "\n",
    "- Multi-Head Self-Attention layer\n",
    "- Add & Norm (Residual connection and Layer Normalization)\n",
    "- Position-wise Feed-Forward Network layer\n",
    "- Add & Norm (Residual connection and Layer Normalization)\n",
    "\n",
    "In the Transformer, multiple encoder blocks (6 according to the paper) are stacked on top of each other to form the complete encoder module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderBlock module.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): The dimensionality of the input.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
    "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model,num_heads)\n",
    "        self.norm1 = AddNorm(d_model)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = AddNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x , mask = None):\n",
    "        x1 = self.self_attn(x,x,x,mask)\n",
    "        x = self.norm1(x,x1)\n",
    "        x1 = self.ffn(x)\n",
    "        x = self.norm2(x,x1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The Decoder Block in the Transformer architecture consists of the following layers:\n",
    "\n",
    "- Masked Multi-Head Self-Attention layer\n",
    "- Add & Norm (Residual connection and Layer Normalization)\n",
    "- Encoder-Decoder Multi-Head Attention layer\n",
    "- Add & Norm (Residual connection and Layer Normalization)\n",
    "- Position-wise Feed-Forward Network layer\n",
    "- Add & Norm (Residual connection and Layer Normalization)\n",
    "\n",
    "In the Transformer, multiple decoder blocks are stacked on top of each other to form the complete decoder module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the DecoderBlock module.\n",
    "\n",
    "        Args:\n",
    "        d_model (int): The dimensionality of the input.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
    "        dropout (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # 1. Masked Multi-Head Self-Attention (causal)\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = AddNorm(d_model)\n",
    "\n",
    "        # 2. Encoder-Decoder Multi-Head Attention\n",
    "        self.enc_dec_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm2 = AddNorm(d_model)\n",
    "\n",
    "        # 3. Feed Forward Network\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = AddNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "        x (Tensor): Target sequence input (batch_size, tgt_seq_len, d_model)\n",
    "        enc_output (Tensor): Encoder output (batch_size, src_seq_len, d_model)\n",
    "        src_mask (Tensor, optional): Encoder mask. Shape (batch_size, 1, 1, src_seq_len)\n",
    "        tgt_mask (Tensor, optional): Decoder mask. Shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Masked Self-Attention with target mask\n",
    "        attn1 = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x, attn1)  # Residual + LayerNorm\n",
    "\n",
    "        # 2. Encoder-Decoder Attention with source mask\n",
    "        attn2 = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x, attn2)\n",
    "\n",
    "        # 3. Feed Forward Network\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x, ffn_out)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "Now that we have implemented all the building blocks, let's assemble the complete Transformer architecture.\n",
    "\n",
    "We initialize the following components:\n",
    "\n",
    "- Source and target embedding layers\n",
    "- Positional encoding module\n",
    "- Encoder and decoder layer stacks\n",
    "- Final linear layer to produce the probability distribution over the target vocabulary\n",
    "\n",
    "In the forward method, we first pass the source and target input tensors through their respective embedding layers and add the positional encoding. Then, we pass the source input through each encoder layer sequentially, followed by passing the target input and encoder output through each decoder layer sequentially. Finally, we apply the linear layer to produce the output tensor with shape (batch_size, tgt_seq_length, tgt_vocab_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, D_MODEL, num_heads, d_ff, max_seq_len, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer module.\n",
    "\n",
    "        Args:\n",
    "        src_vocab_size (int): The size of the source vocabulary.\n",
    "        tgt_vocab_size (int): The size of the target vocabulary.\n",
    "        d_model (int): The dimensionality of the embedding\n",
    "        num_heads (int): The number of attention heads.\n",
    "        d_ff (int): The dimensionality of the hidden layer in the feed-forward network.\n",
    "        max_seq_len (int): The maximum length of the input sequence.\n",
    "        num_layers (int): The number of layers in the encoder and decoder.\n",
    "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, D_MODEL)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, D_MODEL)\n",
    "        self.pos_encoding = PositionalEncoding(D_MODEL, max_seq_len, dropout)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(D_MODEL, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(D_MODEL, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer.\n",
    "\n",
    "        Args:\n",
    "        src (Tensor): The source input tensor of shape (batch_size, src_seq_length).\n",
    "        tgt (Tensor): The target input tensor of shape (batch_size, tgt_seq_length).\n",
    "        src_mask (Tensor, optional): The source mask tensor for ignoring certain elements. Defaults to None.\n",
    "        tgt_mask (Tensor, optional): The target mask tensor for ignoring certain elements. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output tensor of shape (batch_size, tgt_seq_length, tgt_vocab_size).\n",
    "        \"\"\"\n",
    "        src = self.src_embedding(src)\n",
    "        src = self.pos_encoding(src)\n",
    "\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        tgt = self.pos_encoding(tgt)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
    "\n",
    "        out = self.fc(tgt)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpatches\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize_transformer\u001b[39m(num_encoder_layers, num_decoder_layers, num_heads, d_model):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def visualize_transformer(num_encoder_layers, num_decoder_layers, num_heads, d_model):\n",
    "    \"\"\"\n",
    "    Visualizes a high-level architecture of the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        num_encoder_layers (int): Number of encoder layers.\n",
    "        num_decoder_layers (int): Number of decoder layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_model (int): Embedding dimension.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Encoder blocks\n",
    "    encoder_x = 1\n",
    "    encoder_y_start = 1\n",
    "    encoder_height = 0.6\n",
    "    encoder_gap = 0.15\n",
    "    for i in range(num_encoder_layers):\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (encoder_x, encoder_y_start + i * (encoder_height + encoder_gap)),\n",
    "            1.2, encoder_height,\n",
    "            boxstyle=\"round,pad=0.02\",\n",
    "            edgecolor='navy', facecolor='#cce5ff', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(encoder_x + 0.6, encoder_y_start + i * (encoder_height + encoder_gap) + encoder_height/2,\n",
    "                f'Encoder Layer {i+1}\\n(Multi-Head x{num_heads})', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Decoder blocks\n",
    "    decoder_x = 5\n",
    "    decoder_y_start = 1\n",
    "    decoder_height = 0.6\n",
    "    decoder_gap = 0.15\n",
    "    for i in range(num_decoder_layers):\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (decoder_x, decoder_y_start + i * (decoder_height + decoder_gap)),\n",
    "            1.2, decoder_height,\n",
    "            boxstyle=\"round,pad=0.02\",\n",
    "            edgecolor='darkgreen', facecolor='#d4edda', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(decoder_x + 0.6, decoder_y_start + i * (decoder_height + decoder_gap) + decoder_height/2,\n",
    "                f'Decoder Layer {i+1}\\n(Multi-Head x{num_heads})', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Embedding and Positional Encoding\n",
    "    ax.text(encoder_x - 0.7, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2,\n",
    "            'Input\\nEmbedding\\n+\\nPositional\\nEncoding', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#f8d7da\", ec=\"crimson\"))\n",
    "    ax.text(decoder_x - 0.7, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2,\n",
    "            'Output\\nEmbedding\\n+\\nPositional\\nEncoding', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#fff3cd\", ec=\"#856404\"))\n",
    "\n",
    "    # Output Linear Layer\n",
    "    ax.text(decoder_x + 2.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2,\n",
    "            'Linear\\n& Softmax', ha='center', va='center', fontsize=11, bbox=dict(boxstyle=\"round\", fc=\"#d1ecf1\", ec=\"#0c5460\"))\n",
    "\n",
    "    # Arrows: Input -> Encoder\n",
    "    ax.annotate('', xy=(encoder_x, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                xytext=(encoder_x - 0.2, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Arrows: Encoder -> Decoder (cross attention)\n",
    "    ax.annotate('', xy=(encoder_x + 1.2, encoder_y_start + (num_encoder_layers * (encoder_height + encoder_gap))/2),\n",
    "                xytext=(decoder_x, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='gray', arrowstyle='->', lw=2, linestyle='dashed'))\n",
    "\n",
    "    # Arrows: Output Embedding -> Decoder\n",
    "    ax.annotate('', xy=(decoder_x, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                xytext=(decoder_x - 0.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Arrows: Decoder -> Linear\n",
    "    ax.annotate('', xy=(decoder_x + 1.2, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                xytext=(decoder_x + 2.0, decoder_y_start + (num_decoder_layers * (decoder_height + decoder_gap))/2),\n",
    "                arrowprops=dict(facecolor='black', arrowstyle='->', lw=2))\n",
    "\n",
    "    # Title and legend\n",
    "    ax.set_title(f\"Transformer Architecture\\n(Encoder Layers: {num_encoder_layers}, Decoder Layers: {num_decoder_layers}, Heads: {num_heads}, d_model: {d_model})\", fontsize=14, pad=20)\n",
    "    plt.xlim(0, 8)\n",
    "    plt.ylim(0, 3 + max(num_encoder_layers, num_decoder_layers) * (encoder_height + encoder_gap) / 2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "visualize_transformer(num_encoder_layers=6, num_decoder_layers=6, num_heads=8, d_model=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the **Transformer Architecture**, a foundational model used in many modern NLP systems such as BERT, GPT, and T5. It visualizes how information flows through the **encoder-decoder structure** of a transformer.\n",
    "\n",
    "Let‚Äôs break it down **intuitively and step by step**:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† High-Level Summary\n",
    "\n",
    "* The **left side** is the **Encoder**, which reads and understands the input.\n",
    "* The **right side** is the **Decoder**, which generates the output (like a translated sentence).\n",
    "* In between, the encoder and decoder **communicate** using attention mechanisms.\n",
    "* The **transformer uses 8 attention heads** and **6 stacked layers** in both encoder and decoder, with a model dimension (`d_model`) of 512.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¥ Step 1: Input Embedding + Positional Encoding\n",
    "\n",
    "* Each word/token in the input sequence is **converted to a dense vector (embedding)**.\n",
    "* Since transformers don‚Äôt understand order, **positional encoding** is added to inject the notion of word order (e.g., who comes first in the sentence).\n",
    "\n",
    "üì¶ Example:\n",
    "\n",
    "> `\"I love transformers\"` ‚Üí becomes a matrix of shape `(seq_len, 512)`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Encoder Stack (Left Side)\n",
    "\n",
    "There are **6 identical encoder layers**, each with:\n",
    "\n",
    "1. **Multi-head self-attention**\n",
    "   ‚Üí Every word looks at every other word (including itself) to understand context.\n",
    "\n",
    "   > e.g., ‚Äúbank‚Äù can mean money or river ‚Äî attention helps disambiguate it by context.\n",
    "\n",
    "2. **Feedforward network**\n",
    "   ‚Üí A small neural network to refine each word‚Äôs representation.\n",
    "\n",
    "3. **Residual connections + LayerNorm**\n",
    "   ‚Üí Helps in stabilizing training and preserving input signals.\n",
    "\n",
    "üìå Output: A context-enriched representation for each word.\n",
    "\n",
    "---\n",
    "\n",
    "## üü° Output Embedding + Positional Encoding\n",
    "\n",
    "This is for the **decoder input** (often previous tokens during training or inference).\n",
    "\n",
    "* The decoder also needs **positional info**.\n",
    "* It uses **shifted right** sequences during training (i.e., we don't feed the full output at once, only up to the current word).\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ Decoder Stack (Right Side)\n",
    "\n",
    "Also has **6 layers**, and each contains:\n",
    "\n",
    "1. **Masked Multi-head self-attention**\n",
    "   ‚Üí Each position can only attend to previous tokens (to prevent cheating during generation).\n",
    "\n",
    "2. **Encoder-Decoder attention**\n",
    "   ‚Üí The decoder attends to encoder outputs ‚Äî this is how the decoder knows what the input meant.\n",
    "\n",
    "3. **Feedforward network**\n",
    "   ‚Üí Like in the encoder, applies transformation to each position.\n",
    "\n",
    "Each layer builds a **richer representation** of the output sequence being generated.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Final Step: Linear + Softmax\n",
    "\n",
    "* After decoder layers, the final output goes through a **linear layer** followed by **softmax** to predict the next word.\n",
    "* This output is a probability distribution over the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Example: English to French Translation\n",
    "\n",
    "```text\n",
    "Input: \"I love transformers\"\n",
    "‚Üì\n",
    "Encoder processes this and creates contextual embeddings\n",
    "‚Üì\n",
    "Decoder begins with: \"<start>\" token\n",
    "‚Üì\n",
    "Decoder predicts \"J'\"\n",
    "‚Üì\n",
    "Then uses \"J'\" + context to predict \"aime\"\n",
    "‚Üì\n",
    "Repeats until \"<end>\" is generated\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Config Summary (from diagram):\n",
    "\n",
    "* `Encoder Layers: 6`\n",
    "* `Decoder Layers: 6`\n",
    "* `Heads: 8` (each layer has 8 attention heads)\n",
    "* `d_model: 512` (embedding size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29001/29001 [00:00<00:00, 36164.77it/s]\n",
      "Building vocab: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29001/29001 [00:01<00:00, 23170.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab Size English 10837\n",
      "\n",
      "Vocab Size German 19214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import Multi30k\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def ensure_spacy_model(model_name):\n",
    "    try:\n",
    "        spacy.load(model_name)\n",
    "    except OSError:\n",
    "        print(f\"Downloading spaCy model '{model_name}'...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\n",
    "\n",
    "# Ensure required spaCy models are installed\n",
    "ensure_spacy_model(\"en_core_web_sm\")\n",
    "ensure_spacy_model(\"de_core_news_sm\")\n",
    "\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "de_tokenizer = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return en_tokenizer(str(text))\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return de_tokenizer(str(text))\n",
    "\n",
    "# --- FIXED DATA LOADING ---\n",
    "train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
    "train_data_en = []\n",
    "train_data_de = []\n",
    "for en, de in train_iter:\n",
    "    train_data_en.append(en)\n",
    "    train_data_de.append(de)\n",
    "# --- END FIX ---\n",
    "\n",
    "class VOCAB:\n",
    "    def __init__(self, tokenizer, min_freq=2, data=None, special_tokens=['<pad>', '<sos>', '<eos>', '<unk>']):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_freq = min_freq\n",
    "        self.special_tokens = special_tokens\n",
    "        self.build_vocab(data)\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        counter = Counter()\n",
    "        for text in tqdm(data, desc=\"Building vocab\"):\n",
    "            tokens = self.tokenizer(text)\n",
    "            counter.update(tokens)\n",
    "        tokens = [token for token, freq in counter.items() if freq >= self.min_freq and token not in self.special_tokens]\n",
    "        tokens = self.special_tokens + tokens\n",
    "        self.stoi = {token: index for index, token in enumerate(tokens)}\n",
    "        self.itos = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi['<unk>'])\n",
    "\n",
    "EN_VOCAB = VOCAB(tokenize_en, min_freq=1, data=train_data_en)\n",
    "DE_VOCAB = VOCAB(tokenize_de, min_freq=1, data=train_data_de)\n",
    "print(\"\\nVocab Size English\", len(EN_VOCAB))\n",
    "print(\"\\nVocab Size German\", len(DE_VOCAB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code prepares bilingual text data (English ‚Üî German) for a machine translation model using the **Multi30k dataset**.\n",
    "\n",
    "Here's what it does:\n",
    "\n",
    "* ‚úÖ **Loads English‚ÄìGerman sentence pairs** using `torchtext`.\n",
    "* üß† **Tokenizes** the sentences using **spaCy**, a language-aware tokenizer that handles grammar, punctuation, and morphology.\n",
    "* üì¶ **Builds vocabularies** for both languages:\n",
    "\n",
    "  * Assigns each token a unique index.\n",
    "  * Includes special tokens like `<pad>`, `<sos>`, `<eos>`, and `<unk>`.\n",
    "  * Filters out rare words (optional via `min_freq`).\n",
    "* üî¢ The final output is a mapping from words ‚Üí numbers, which is essential for training neural networks.\n",
    "\n",
    "This setup forms the **first step in building a translation model**‚Äîconverting raw text into something a neural model can understand and learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, en_data, de_data, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with source (English) and target (German) data,\n",
    "        along with their respective tokenizers and vocabularies.\n",
    "        \"\"\"\n",
    "        self.en_data = en_data  # List of English sentences\n",
    "        self.de_data = de_data  # List of German sentences\n",
    "        self.src_tokenizer = src_tokenizer  # Tokenizer for English\n",
    "        self.tgt_tokenizer = tgt_tokenizer  # Tokenizer for German\n",
    "        self.src_vocab = src_vocab  # English vocabulary (token -> index)\n",
    "        self.tgt_vocab = tgt_vocab  # German vocabulary (token -> index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Process a single (English, German) sentence pair:\n",
    "        - Tokenize\n",
    "        - Convert tokens to indices\n",
    "        - Add <sos> and <eos> special tokens\n",
    "        - Return tensors for both source and target\n",
    "        \"\"\"\n",
    "        src_txt, tgt_txt = self.en_data[index], self.de_data[index]\n",
    "\n",
    "        # Tokenize and convert to indices using vocab\n",
    "        src_tokens = [self.src_vocab[token] for token in self.src_tokenizer(src_txt)]\n",
    "        tgt_tokens = [self.tgt_vocab[token] for token in self.tgt_tokenizer(tgt_txt)]\n",
    "\n",
    "        # Add <sos> and <eos> tokens around the sequences\n",
    "        src_tokens = [self.src_vocab['<sos>']] + src_tokens + [self.src_vocab['<eos>']]\n",
    "        tgt_tokens = [self.tgt_vocab['<sos>']] + tgt_tokens + [self.tgt_vocab['<eos>']]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        src_tensor = torch.LongTensor(src_tokens)\n",
    "        tgt_tensor = torch.LongTensor(tgt_tokens)\n",
    "\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of sentence pairs in the dataset.\n",
    "        \"\"\"\n",
    "        assert len(self.en_data) == len(self.de_data)  # Ensure aligned data\n",
    "        return len(self.en_data)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom function to pad batches of variable-length sequences:\n",
    "        - Pads all source and target tensors in the batch to the same length\n",
    "        - Uses <pad> token index from vocab\n",
    "        - Returns two padded tensors (src_batch, tgt_batch)\n",
    "        \"\"\"\n",
    "        src_tensors, tgt_tensors = zip(*batch)  # Unzip list of (src, tgt) pairs\n",
    "\n",
    "        # Pad sequences to match longest in batch (for batching)\n",
    "        src_tensors = torch.nn.utils.rnn.pad_sequence(\n",
    "            src_tensors, padding_value=self.src_vocab['<pad>'], batch_first=True\n",
    "        )\n",
    "        tgt_tensors = torch.nn.utils.rnn.pad_sequence(\n",
    "            tgt_tensors, padding_value=self.tgt_vocab['<pad>'], batch_first=True\n",
    "        )\n",
    "\n",
    "        return src_tensors, tgt_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct Multi30k data folder and file names as per the multi30k_data folder and the 2016 split\n",
    "\n",
    "import gzip\n",
    "\n",
    "def read_gzipped_lines(filepath):\n",
    "    with gzip.open(filepath, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "# Paths to gzipped files in the multi30k_data folder\n",
    "train_en_file = \"multi30k/train.en.gz\"\n",
    "train_de_file = \"multi30k/train.de.gz\"\n",
    "val_en_file   = \"multi30k/val.en.gz\"\n",
    "val_de_file   = \"multi30k/val.de.gz\"\n",
    "test_en_file  = \"multi30k/test_2016_flickr.en.gz\"\n",
    "test_de_file  = \"multi30k/test_2016_flickr.de.gz\"\n",
    "\n",
    "# Read the data from gzipped files\n",
    "train_data_en = read_gzipped_lines(train_en_file)\n",
    "train_data_de = read_gzipped_lines(train_de_file)\n",
    "val_data_en   = read_gzipped_lines(val_en_file)\n",
    "val_data_de   = read_gzipped_lines(val_de_file)\n",
    "test_data_en  = read_gzipped_lines(test_en_file)\n",
    "test_data_de  = read_gzipped_lines(test_de_file)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data_en, train_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "val_dataset = TranslationDataset(val_data_en, val_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "test_dataset = TranslationDataset(test_data_en, test_data_de, tokenize_en, tokenize_de, EN_VOCAB, DE_VOCAB)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  2]),\n",
       " tensor([ 1,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,  2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> Two young , White males are outside near many bushes . <eos>',\n",
       " '<sos> Zwei junge wei√üe M√§nner sind im Freien in der N√§he vieler B√ºsche . <eos>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN_VOCAB.itos[i] for i in train_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in train_dataset[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A man in an orange hat starring at something.',\n",
       " 'Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_en[0], test_data_de[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<sos> A man in an orange hat starring at something . <eos>',\n",
       " '<sos> Ein Mann mit einem orangefarbenen Hut , der etwas <unk> . <eos>')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([EN_VOCAB.itos[i] for i in test_dataset[0][0]]), ' '.join([DE_VOCAB.itos[i] for i in test_dataset[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model and associated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hyperparameters\n",
    "# Define Hyper Parameters\n",
    "NUM_EPOCHS      = 20\n",
    "D_MODEL         = 256\n",
    "ATTN_HEADS      = 8\n",
    "NUM_LAYERS      = 3\n",
    "FEEDFORWARD_DIM = 512\n",
    "DROPOUT         = 0.1\n",
    "MAX_SEQ_LEN     = 150\n",
    "SRC_VOCAB_SIZE  = len(EN_VOCAB)\n",
    "TGT_VOCAB_SIZE  = len(DE_VOCAB)\n",
    "LR              = 0\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class NoamScheduler:\n",
    "    def __init__(self,optimizer,d_model, warmup_steps = 4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.learning_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def learning_rate(self):\n",
    "        step = self.current_step\n",
    "        # Add a small epsilon to avoid division by zero if step is 0\n",
    "        return (self.d_model ** -0.5) * min((step + 1e-9) ** -0.5, step * self.warmup_steps ** -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, ATTN_HEADS, FEEDFORWARD_DIM, MAX_SEQ_LEN, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "# optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9, weight_decay=5e-2)\n",
    "warmup_steps = 2 * len(train_dataloader)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "# scheduler = LambdaLR(optimizer, lr_lambda=lambda step: (D_MODEL ** -0.5) * min((step + 1) ** -0.5, (step + 1) * warmup_steps ** -1.5), verbose=True)\n",
    "scheduler = NoamScheduler(optimizer, d_model=D_MODEL, warmup_steps=warmup_steps)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=DE_VOCAB['<pad>'], label_smoothing=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "\n",
    "import sacrebleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def generate_tgt_mask(tgt, pad_idx):\n",
    "    seq_len = tgt.size(1)\n",
    "    no_future_mask = torch.tril(torch.ones((seq_len, seq_len), device=DEVICE)).bool()\n",
    "    pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    combined_mask = pad_mask & no_future_mask\n",
    "    return combined_mask\n",
    "\n",
    "def generate_src_mask(src, pad_idx):\n",
    "    mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "    return mask\n",
    "\n",
    "def calculate_bleu(tgt_output, output):\n",
    "    tgt_output = tgt_output.cpu().numpy()\n",
    "    output = output.cpu().numpy()\n",
    "\n",
    "    refs = []\n",
    "    hyps = []\n",
    "\n",
    "    for tgt, pred in zip(tgt_output, output):\n",
    "        ref = ' '.join([DE_VOCAB.itos[t] for t in tgt if t not in (DE_VOCAB['<pad>'], DE_VOCAB['<eos>'], DE_VOCAB['<sos>'])])\n",
    "        hyp = ' '.join([DE_VOCAB.itos[t] for t in pred if t not in (DE_VOCAB['<pad>'], DE_VOCAB['<eos>'], DE_VOCAB['<sos>'])])\n",
    "\n",
    "        refs.append(ref)\n",
    "        hyps.append(hyp)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs], force=True).score\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # print(\"Src\", src.shape, \"Tgt\", tgt.shape)\n",
    "        src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            loss = criterion(output.reshape(-1, output.size(2)), tgt_output.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(total_loss / (i + 1)),\n",
    "            lr=\"{:.09f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, DEVICE):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_bleu_score = 0\n",
    "\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc='Validate')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(dataloader):\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "            tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "                loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_bleu_score += calculate_bleu(tgt_output, output.argmax(-1))\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(epoch_loss / (i + 1)),\n",
    "            bleu=\"{:.04f}\".format(epoch_bleu_score / (i + 1)))\n",
    "\n",
    "            batch_bar.update()\n",
    "\n",
    "    # Normalize the loss and BLEU score by the number of validation samples\n",
    "    epoch_loss /= len(dataloader)\n",
    "    epoch_bleu_score /= len(dataloader)\n",
    "\n",
    "    return epoch_loss, epoch_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, src, de_tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    src_mask = generate_src_mask(src, EN_VOCAB['<pad>'])\n",
    "\n",
    "    # Initialize target input tensors with <sos> tokens\n",
    "    tgt_input = torch.full((src.size(0), 1), DE_VOCAB['<sos>'], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # Create a flag for each sequence in the batch\n",
    "    eos_flags = torch.zeros(src.size(0), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    # Perform inference for each target token\n",
    "    with torch.no_grad():\n",
    "        for _ in range(70):\n",
    "            tgt_mask = generate_tgt_mask(tgt_input, DE_VOCAB['<pad>'])\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            next_tokens = output.argmax(2)[:, -1].unsqueeze(1)\n",
    "            tgt_input = torch.cat((tgt_input, next_tokens), dim=1)\n",
    "\n",
    "            # Update the eos_flags for sequences that have generated <eos>\n",
    "            eos_flags |= (next_tokens.squeeze() == DE_VOCAB['<eos>'])\n",
    "\n",
    "            # Stop generating tokens if all sequences have generated <eos> or reached maximum length\n",
    "            if torch.all(eos_flags):\n",
    "                break\n",
    "\n",
    "    # Convert target input tensors to translated sentences\n",
    "    translated_sentences = []\n",
    "    for i in range(tgt_input.size(0)):\n",
    "        translated_tokens = []\n",
    "        for token in tgt_input[i][1:]:\n",
    "            if token == DE_VOCAB['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                translated_tokens.append(DE_VOCAB.itos[token.item()])\n",
    "        translated_sentence = ' '.join(translated_tokens)\n",
    "        translated_sentences.append(translated_sentence)\n",
    "    return translated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embedding): Embedding(10837, 256)\n",
      "  (tgt_embedding): Embedding(19214, 256)\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-2): 3 x EncoderBlock(\n",
      "      (self_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm2): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-2): 3 x DecoderBlock(\n",
      "      (self_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm1): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (enc_dec_attn): MultiHeadSelfAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm2): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm3): AddNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=19214, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print a summary of the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8258 | Val Loss: 4.2103 | BLEU Score: 6.6498\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8228 | Val Loss: 3.6152 | BLEU Score: 10.5960\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3153 | Val Loss: 3.3305 | BLEU Score: 14.9178\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9369 | Val Loss: 3.1557 | BLEU Score: 14.2710\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6769 | Val Loss: 3.0911 | BLEU Score: 16.9041\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4787 | Val Loss: 3.0350 | BLEU Score: 19.7898\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3186 | Val Loss: 3.0254 | BLEU Score: 20.5710\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1906 | Val Loss: 2.9974 | BLEU Score: 27.9643\n",
      "‚úÖ Best model saved.\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0868 | Val Loss: 3.0169 | BLEU Score: 19.5570\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0041 | Val Loss: 3.0241 | BLEU Score: 19.3396\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9401 | Val Loss: 3.0485 | BLEU Score: 19.6496\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8880 | Val Loss: 3.0643 | BLEU Score: 20.1703\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8431 | Val Loss: 3.0741 | BLEU Score: 20.1040\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8054 | Val Loss: 3.1011 | BLEU Score: 19.0616\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7747 | Val Loss: 3.1071 | BLEU Score: 17.7373\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7470 | Val Loss: 3.1435 | BLEU Score: 18.8883\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7211 | Val Loss: 3.1534 | BLEU Score: 18.3990\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7001 | Val Loss: 3.1793 | BLEU Score: 23.1467\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6813 | Val Loss: 3.1750 | BLEU Score: 28.2455\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6653 | Val Loss: 3.1796 | BLEU Score: 23.5873\n",
      "\n",
      "Training complete. Best validation loss: 2.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Make sure the following are defined before running:\n",
    "# NUM_EPOCHS, model, optimizer, criterion, DEVICE, scheduler (optional)\n",
    "# train_epoch, validate_epoch, train_dataloader, val_dataloader\n",
    "\n",
    "def main():\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "        # ----- Training -----\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, DEVICE)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ----- Validation -----\n",
    "        val_loss, bleu_score = validate_epoch(model, val_dataloader, criterion, DEVICE)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # ----- Print epoch summary -----\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "        # ----- Save the best model -----\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"‚úÖ Best model saved.\")\n",
    "\n",
    "        # ----- Step the scheduler if available -----\n",
    "        if 'scheduler' in locals() and scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    # Save loss/metric history\n",
    "    np.save(\"train_losses.npy\", np.array(train_losses))\n",
    "    np.save(\"val_losses.npy\", np.array(val_losses))\n",
    "    np.save(\"bleu_scores.npy\", np.array(bleu_scores))\n",
    "    print(f\"\\nTraining complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:50<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example Sentence and its Translation\n",
      "Source Sentence in English               : A female performer with a violin plays on a street while a woman with a blue guitar looks on .\n",
      "German have the truth          : Eine <unk> mit einer Violine spielt auf der Stra√üe w√§hrend eine Frau mit einer blauen Gitarre zusieht .\n",
      "Machine Translated Sentence in German    : Eine K√ºnstlerin spielt auf einer Stra√üe mit einer Violine , w√§hrend eine Frau mit einer blauen Gitarre zuschaut .\n",
      "Test BLEU score: 54.91004867761124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def evaluate_test_set_bleu(model, test_dataloader, de_tokenizer):\n",
    "    translated_sentences = []\n",
    "    ground_truth_sentences = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "        src, tgt_output = batch\n",
    "        src, tgt = src.to(DEVICE), tgt_output.to(DEVICE)\n",
    "        tgt_sentences = [' '.join([DE_VOCAB.itos[token.item()] for token in sequence if token.item() not in [DE_VOCAB['<pad>'], DE_VOCAB['<sos>'], DE_VOCAB['<eos>']]]) for sequence in tgt_output]\n",
    "\n",
    "        translations = inference(model, src, de_tokenizer)\n",
    "        translated_sentences.extend(translations)\n",
    "        ground_truth_sentences.extend([[tgt] for tgt in tgt_sentences])\n",
    "\n",
    "    rand_index = random.randint(0, len(test_dataset))\n",
    "    print(\"\\n\\nExample Sentence and its Translation\")\n",
    "    print(\"Source Sentence in English               :\", ' '.join([EN_VOCAB.itos[i] for i in test_dataset[rand_index][0] if EN_VOCAB.itos[i] not in ['<pad>', '<sos>', '<eos>']]))\n",
    "    print(\"German have the truth          :\", ground_truth_sentences[rand_index][0])\n",
    "    print(\"Machine Translated Sentence in German    :\", translated_sentences[rand_index])\n",
    "    bleu_score = sacrebleu.corpus_bleu(translated_sentences, ground_truth_sentences)\n",
    "    return bleu_score\n",
    "\n",
    "# Usage example\n",
    "test_bleu = evaluate_test_set_bleu(model, test_dataloader, de_tokenizer)\n",
    "print(\"Test BLEU score:\", test_bleu.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
